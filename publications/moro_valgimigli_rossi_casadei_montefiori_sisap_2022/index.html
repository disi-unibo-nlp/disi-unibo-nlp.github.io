<!doctype html>




<html
    dir="ltr"
    lang="en"
    class=" "
>
    <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#ffffff">
    <link rel="stylesheet" href="/assets/css/app.css">
    <link
        rel="shortcut icon"
        type="image/png"
        
            href="/favicon.png"
        
    >
    <script defer src="https://unpkg.com/alpinejs@3.9.0/dist/cdn.min.js"></script>
    <script
        src="https://cdn.jsdelivr.net/npm/js-cookie@3.0.5/dist/js.cookie.min.js"
    ></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">
    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-social@1/bin/bulma-social.min.css">
    
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Self-supervised Information Retrieval Trained from Self-generated Sets of Queries and Relevant Documents | UniboNLP</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Self-supervised Information Retrieval Trained from Self-generated Sets of Queries and Relevant Documents" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The Unibo Natural Language Processing Group (UniboNLP), led by prof. Gianluca Moro, includes a large team of Ph.D. students and researchers which are part of the Department of Computer Science and Engineering (DISI) of the University of Bologna." />
<meta property="og:description" content="The Unibo Natural Language Processing Group (UniboNLP), led by prof. Gianluca Moro, includes a large team of Ph.D. students and researchers which are part of the Department of Computer Science and Engineering (DISI) of the University of Bologna." />
<link rel="canonical" href="http://nlp.apice.unibo.it/publications/moro_valgimigli_rossi_casadei_montefiori_sisap_2022/" />
<meta property="og:url" content="http://nlp.apice.unibo.it/publications/moro_valgimigli_rossi_casadei_montefiori_sisap_2022/" />
<meta property="og:site_name" content="UniboNLP" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-07-23T11:08:52+00:00" />
<script type="application/ld+json">
{"description":"The Unibo Natural Language Processing Group (UniboNLP), led by prof. Gianluca Moro, includes a large team of Ph.D. students and researchers which are part of the Department of Computer Science and Engineering (DISI) of the University of Bologna.","url":"http://nlp.apice.unibo.it/publications/moro_valgimigli_rossi_casadei_montefiori_sisap_2022/","@type":"BlogPosting","headline":"Self-supervised Information Retrieval Trained from Self-generated Sets of Queries and Relevant Documents","dateModified":"2024-07-23T11:08:52+00:00","datePublished":"2024-07-23T11:08:52+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://nlp.apice.unibo.it/publications/moro_valgimigli_rossi_casadei_montefiori_sisap_2022/"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <!-- head scripts -->
</head>

    <body>
        
        <nav
    class="navbar is-primary "
    x-data="{ openNav: false }"
>
    <div class="container">
        <div class="navbar-brand">
            <a href="/" class="navbar-item">
                UniboNLP
            </a>
            <a
                role="button"
                class="navbar-burger burger"
                aria-label="menu"
                aria-expanded="false"
                data-target="navMenu"
                :class="{ 'is-active': openNav }"
                x-on:click="openNav = !openNav"
            >
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu" id="navMenu" :class="{ 'is-active': openNav }">
            <div class="navbar-start">
                <a href="/" class="navbar-item ">Home</a>
                
                    
                        
                            <a
                                href="/people/"
                                class="navbar-item "
                            >People</a>
                        
                    
                        
                            <a
                                href="/publications/"
                                class="navbar-item "
                            >Publications</a>
                        
                    
                        
                            <a
                                href="/projects/"
                                class="navbar-item "
                            >Projects</a>
                        
                    
                        
                            <a
                                href="/theses/"
                                class="navbar-item "
                            >Theses</a>
                        
                    
                
            </div>

            <div class="navbar-end">
                
            </div>
        </div>
    </div>
</nav>

        
            <section
    class="hero  is-medium  is-bold is-primary"
    
>
    <div class="hero-body ">
        <div class="container">
            <h1 class="title is-2">Self-supervised Information Retrieval Trained from Self-generated Sets of Queries and Relevant Documents</h1>
            <p class="subtitle is-3"></p>
            
        </div>
    </div>
</section>

        
        

        <section class="section">
            <div class="container">
                <div class="columns is-multiline">
                    
                    <div class="column is-12">
                        

                        

                        

                        

                        <div class="columns is-multiline">
    
    <div class="column is-12">
        

    </div>

    <!-- Group publications by year (descending order) -->
    
    

    <!-- Only way to create a list in Liquid is to split strings -->
    

    
    
    <!-- Button menus for desktop -->
    <!-- Buttons for publication year -->
    
    <div class="column is-half is-offset-one-quarter">
        <p class="control">
            <input class="input search-input" type="text" id="publication-search" placeholder="Search by title or author..." />
        </p>
    </div>
    
    <div class="filter-container column is-8">
        <div class="filter-buttons">
            <button class="filter-button active" data-year="all">All</button>
            <button class="filter-button" data-year="2024">2024</button>
            <button class="filter-button" data-year="2023">2023</button>
            <button class="filter-button" data-year="2022">2022</button>
            <button class="filter-button" data-year="2021">2021</button>
            <button class="filter-button" data-year="2020">2020</button>
            <div class="filter-buttons-type" style="margin-left:30px;">
                <button class="filter-button active" data-type="all-type">All</button>
                <button class="filter-button" data-type="conference">Conference</button>
                <button class="filter-button" data-type="journal">Journal</button>
            </div>
        </div>
    </div>
    
    <div class="filter-dropdowns">
        <!-- Dropdown options for publication year -->
        <select class="filter-dropdown" id="year-dropdown">
            <option class="active" data-year="all">All Years</option>
            <option data-year="2024">2024</option>
            <option data-year="2023">2023</option>
            <option data-year="2022">2022</option>
            <option data-year="2021">2021</option>
            <option data-year="2020">2020</option>
        </select>
        <!-- Dropdown options for publication type -->
        <select class="filter-dropdown" id="type-dropdown">
            <option class="active" data-type="all-type">All Types</option>
            <option data-type="conference">Conference</option>
            <option data-type="journal">Journal</option>
        </select>
    </div>

    <div class="column is-2">
        <div class="q1-checkbox-container">
            <span>
                <input type="checkbox" id="q1-checkbox" class="styled-checkbox">
                <label for="q1-checkbox" class="award-label">A+ / Q1</label>
            </span>
        </div>
    </div>
    
    <div class="column is-2">
        <div class="award-checkbox-container">
            <span>
                <input type="checkbox" id="award-checkbox" class="styled-checkbox">
                <label for="award-checkbox" class="award-label">Award-winning</label>
            </span>
        </div>
    </div>
    
    <div class="column is-3 content">
        <blockquote class="publication-counter" style="padding: 5px 10px;">
            Found <span id="publication-count">0</span> publications
        </blockquote>
    </div>


    <!-- LOOP 1: PUBLICATION TYPE -->
    
    <h2 class="publication-type-header">
        
        Conference Proceedings
        
    </h2>

    <ul class="publication-list">

        <!-- LOOP 2: PUBLICATION YEAR -->
        
        <!-- Within the same year, sort publications by month (descending order) -->
        

        <!-- LOOP 3: PUBLICATIONS (BY MONTH) -->
        
        
        
        
        
    
        <li class="publication-item 2024 conference" id="111">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">What Are You Token About? Differentiable Perturbed Top-k Token Selection for Scientific Document Summarization</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Luca Ragazzi,</span>
                
                <span class="author">Paolo Italiani,</span>
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Mattia Panni</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">ACL 2024</span>
                    <span class="venue_complete">Findings of the 62nd Annual Meeting of the Association for Computational Linguistics</span>
                </div>
                <div class="publication-details  q1 ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <a target="_blank" href="https://github.com/disi-unibo-nlp/sci-lay">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/2428/PNG/512/github_black_logo_icon_147128.png"
                                    alt="GitHub Icon">
                                <span class="label">Code</span>
                            </div>
                        </a>
                        
                        <!-- CITE -->
                        
                        <!-- READ -->
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Scientific document summarization (SDS) aims to condense complex and long articles in both technical and plain-language terms to facilitate the accessibility and dissemination of scientific findings. Existing datasets lack source heterogeneity, hindering effective model training and generalizability. First, we introduce SciLay, a novel dataset that includes documents from multiple natural science journals with expert-authored technical and lay summaries. Second, we propose PrunePert, a new transformer-based model that incorporates a differentiable perturbed top-k encoder layer to prune irrelevant tokens in end-to-end learning. Experimental results show that our model achieves a nearly 2x speed-up compared to a state-of-the-art linear transformer, remaining comparable in effectiveness. Additional examinations underscore the importance of employing a training dataset that includes different sources to enhance the generalizability of the models. Code is available at https://github.com/disi-unibo-nlp/sci-lay. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Scientific document summarization (SDS) aims to condense complex and long articles in both technical and plain-language terms to facilitate the accessibility and dissemination of scientific findings. Existing datasets lack source heterogeneity, hindering effective model training and generalizability. First, we introduce SciLay, a novel dataset that includes documents from multiple natural science journals with expert-authored technical and lay summaries. Second, we propose PrunePert, a new transformer-based model that incorporates a differentiable perturbed top-k encoder layer to prune irrelevant tokens in end-to-end learning. Experimental results show that our model achieves a nearly 2x speed-up compared to a state-of-the-art linear transformer, remaining comparable in effectiveness. Additional examinations underscore the importance of employing a training dataset that includes different sources to enhance the generalizability of the models. Code is available at https://github.com/disi-unibo-nlp/sci-lay. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
    
        <li class="publication-item 2024 conference" id="222">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Giacomo Frisoni,</span>
                
                <span class="author">Alessio Cocchieri,</span>
                
                <span class="author">Alex Presepi,</span>
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Zaiqiao Meng</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">ACL 2024</span>
                    <span class="venue_complete">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>
                </div>
                <div class="publication-details  q1 ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/journals/corr/abs-2403-01924.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://arxiv.org/pdf/2403.01924">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Medical open-domain question answering demands substantial access to specialized knowledge. Recent efforts have sought to decouple knowledge from model parameters, counteracting architectural scaling and allowing for training on common low-resource hardware. The retrieve-then-read paradigm has become ubiquitous, with model predictions grounded on relevant knowledge pieces from external repositories such as PubMed, textbooks, and UMLS. An alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting. As a result, 'to generate or to retrieve' is the modern equivalent of Hamlet’s dilemma. This paper presents MEDGENIE, the first generate-then-read framework for multiple-choice question answering in medicine. We conduct extensive experiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical perspective by assuming a maximum of 24GB VRAM. MEDGENIE sets a new state-of-the- art (SOTA) in the open-book setting of each testbed, even allowing a small-scale reader to outcompete zero-shot closed-book 175B baselines while using up to 706× fewer parameters. Overall, our findings reveal that generated passages are more effective than retrieved counterparts in attaining higher accuracy. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Medical open-domain question answering demands substantial access to specialized knowledge. Recent efforts have sought to decouple knowledge from model parameters, counteracting architectural scaling and allowing for training on common low-resource hardware. The retrieve-then-read paradigm has become ubiquitous, with model predictions grounded on relevant knowledge pieces from external repositories such as PubMed, textbooks, and UMLS. An alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting. As a result, 'to generate or to retrieve' is the modern equivalent of Hamlet’s dilemma. This paper presents MEDGENIE, the first generate-then-read framework for multiple-choice question answering in medicine. We conduct extensive experiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical perspective by assuming a maximum of 24GB VRAM. MEDGENIE sets a new state-of-the- art (SOTA) in the open-book setting of each testbed, even allowing a small-scale reader to outcompete zero-shot closed-book 175B baselines while using up to 706× fewer parameters. Overall, our findings reveal that generated passages are more effective than retrieved counterparts in attaining higher accuracy. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
        
        
        
        
        
    
        <li class="publication-item 2024 conference" id="444">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Revelio: Interpretable Long-Form Question Answering</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Luca Ragazzi,</span>
                
                <span class="author">Lorenzo Valgimigli,</span>
                
                <span class="author">Fabian Vincenzi,</span>
                
                <span class="author">Davide Freddi</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">ICLR 2024</span>
                    <span class="venue_complete">Proceedings of The Second Tiny Papers Track at ICLR 2024</span>
                </div>
                <div class="publication-details  q1 ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <a target="_blank" href="https://github.com/disi-unibo-nlp/revelio">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/2428/PNG/512/github_black_logo_icon_147128.png"
                                    alt="GitHub Icon">
                                <span class="label">Code</span>
                            </div>
                        </a>
                        
                        <!-- CITE -->
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://openreview.net/forum?id=fyvEJXsaQf">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    The black-box architecture of pretrained language models (PLMs) hinders the interpretability of lengthy responses in long-form question answering (LFQA). Prior studies use knowledge graphs (KGs) to enhance output transparency, but mostly focus on non-generative or short-form QA. We present Revelio, a new layer that maps PLM's inner working onto a KG walk. Tests on two LFQA datasets show that Revelio supports PLM-generated answers with reasoning paths presented as rationales while retaining performance and time akin to their vanilla counterparts. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    The black-box architecture of pretrained language models (PLMs) hinders the interpretability of lengthy responses in long-form question answering (LFQA). Prior studies use knowledge graphs (KGs) to enhance output transparency, but mostly focus on non-generative or short-form QA. We present Revelio, a new layer that maps PLM's inner working onto a KG walk. Tests on two LFQA datasets show that Revelio supports PLM-generated answers with reasoning paths presented as rationales while retaining performance and time akin to their vanilla counterparts. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        <!-- Within the same year, sort publications by month (descending order) -->
        

        <!-- LOOP 3: PUBLICATIONS (BY MONTH) -->
        
        
        
        
        
        
        
        
        
        
    
        <li class="publication-item 2023 conference" id="222">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Retrieve-and-Rank End-to-End Summarization of Biomedical Studies</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Luca Ragazzi,</span>
                
                <span class="author">Lorenzo Valgimigli,</span>
                
                <span class="author">Lorenzo Molfetta</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">SISAP 2023</span>
                    <span class="venue_complete">Proceedings of the 16th International Conference on Similarity Search and Applications</span>
                </div>
                <div class="publication-details ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/conf/sisap/MoroRVM23.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-031-46994-7_6">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    An arduous biomedical task involves condensing evidence derived from multiple interrelated studies, given a context as input, to generate reviews or provide answers autonomously. We named this task context-aware multi-document summarization (CA-MDS). Existing state-of-the-art (SOTA) solutions require truncation of the input due to the high memory demands, resulting in the loss of meaningful content. To address this issue effectively, we propose a novel approach called RAMSES, which employs a retrieve-and-rank technique for end-to-end summarization. The model acquires the ability to (i) index each document by modeling its semantic features, (ii) retrieve the most relevant ones, and (iii) generate a summary via token probability marginalization. To facilitate the evaluation, we introduce a new dataset, FAQSUMC19, which includes the synthesizing of multiple supporting papers to answer questions related to Covid-19. Our experimental findings demonstrate that RAMSES achieves notably superior ROUGE scores compared to state-of-the-art methodologies, including the establishment of a new SOTA for the generation of systematic literature reviews using MS2. Quality observation through human evaluation indicates that our model produces more informative responses than previous leading approaches. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    An arduous biomedical task involves condensing evidence derived from multiple interrelated studies, given a context as input, to generate reviews or provide answers autonomously. We named this task context-aware multi-document summarization (CA-MDS). Existing state-of-the-art (SOTA) solutions require truncation of the input due to the high memory demands, resulting in the loss of meaningful content. To address this issue effectively, we propose a novel approach called RAMSES, which employs a retrieve-and-rank technique for end-to-end summarization. The model acquires the ability to (i) index each document by modeling its semantic features, (ii) retrieve the most relevant ones, and (iii) generate a summary via token probability marginalization. To facilitate the evaluation, we introduce a new dataset, FAQSUMC19, which includes the synthesizing of multiple supporting papers to answer questions related to Covid-19. Our experimental findings demonstrate that RAMSES achieves notably superior ROUGE scores compared to state-of-the-art methodologies, including the establishment of a new SOTA for the generation of systematic literature reviews using MS2. Quality observation through human evaluation indicates that our model produces more informative responses than previous leading approaches. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
        
        
        
        
        
    
        <li class="publication-item 2023 conference" id="444">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Graph-based Summarization of Extracted Essential Knowledge for Low-Resource Scenarios</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Luca Ragazzi,</span>
                
                <span class="author">Lorenzo Valgimigli</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">ECAI 2023</span>
                    <span class="venue_complete">Proceedings of the 26th European Conference on Artificial Intelligence</span>
                </div>
                <div class="publication-details ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/conf/ecai/MoroRV23.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://ebooks.iospress.nl/doi/10.3233/FAIA230460">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Although current summarization models can process increasingly long text sequences, they still struggle to capture salient related information spread across the lengthy size of inputs with few labeled training instances. Today’s research still relies on standard input truncation without considering graph-based modeling of multiple semantic units to summarize only crucial facets. This paper proposes G-SEEK, a graph-based summarization of extracted essential knowledge. By representing the long source with a heterogeneous graph, our method extracts and provides salient sentences to an abstractive summarization model to generate the summary. Experimental results in low-resource scenarios, distinguished by data scarcity, reveal that G-SEEK consistently improves both the long- and multi-document summarization performance and accuracy across several datasets. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Although current summarization models can process increasingly long text sequences, they still struggle to capture salient related information spread across the lengthy size of inputs with few labeled training instances. Today’s research still relies on standard input truncation without considering graph-based modeling of multiple semantic units to summarize only crucial facets. This paper proposes G-SEEK, a graph-based summarization of extracted essential knowledge. By representing the long source with a heterogeneous graph, our method extracts and provides salient sentences to an abstractive summarization model to generate the summary. Experimental results in low-resource scenarios, distinguished by data scarcity, reveal that G-SEEK consistently improves both the long- and multi-document summarization performance and accuracy across several datasets. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
    
        <li class="publication-item 2023 conference" id="888">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Carburacy: Summarization Models Tuning and Comparison in Eco-Sustainable Regimes with a Novel Carbon-Aware Accuracy</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Luca Ragazzi,</span>
                
                <span class="author">Lorenzo Valgimigli</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">AAAI 2023</span>
                    <span class="venue_complete">Proceedings of the 37th AAAI Conference on Artificial Intelligence (AAAI 2023)</span>
                </div>
                <div class="publication-details  q1 ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/conf/aaai/MoroRV23.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/view/26686">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Generative transformer-based models have reached cutting-edge performance in long document summarization. Nevertheless, this task is witnessing a paradigm shift in developing ever-increasingly computationally-hungry solutions, focusing on effectiveness while ignoring the economic, environmental, and social costs of yielding such results. Accordingly, such extensive resources impact climate change and raise barriers to small and medium organizations distinguished by low-resource regimes of hardware and data. As a result, this unsustainable trend has lifted many concerns in the community, which directs the primary efforts on the proposal of tools to monitor models' energy costs. Despite their importance, no evaluation measure considering models' eco-sustainability exists yet. In this work, we propose Carburacy, the first carbon-aware accuracy measure that captures both model effectiveness and eco-sustainability. We perform a comprehensive benchmark for long document summarization, comparing multiple state-of-the-art quadratic and linear transformers on several datasets under eco-sustainable regimes. Finally, thanks to Carburacy, we found optimal combinations of hyperparameters that let models be competitive in effectiveness with significantly lower costs. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Generative transformer-based models have reached cutting-edge performance in long document summarization. Nevertheless, this task is witnessing a paradigm shift in developing ever-increasingly computationally-hungry solutions, focusing on effectiveness while ignoring the economic, environmental, and social costs of yielding such results. Accordingly, such extensive resources impact climate change and raise barriers to small and medium organizations distinguished by low-resource regimes of hardware and data. As a result, this unsustainable trend has lifted many concerns in the community, which directs the primary efforts on the proposal of tools to monitor models' energy costs. Despite their importance, no evaluation measure considering models' eco-sustainability exists yet. In this work, we propose Carburacy, the first carbon-aware accuracy measure that captures both model effectiveness and eco-sustainability. We perform a comprehensive benchmark for long document summarization, comparing multiple state-of-the-art quadratic and linear transformers on several datasets under eco-sustainable regimes. Finally, thanks to Carburacy, we found optimal combinations of hyperparameters that let models be competitive in effectiveness with significantly lower costs. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
    
        <li class="publication-item 2023 conference" id="999">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Cogito Ergo Summ: Abstractive Summarization of Biomedical Papers via Semantic Parsing Graphs and Consistency Rewards</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Giacomo Frisoni,</span>
                
                <span class="author">Paolo Italiani,</span>
                
                <span class="author">Stefano Salvatori,</span>
                
                <span class="author">Gianluca Moro</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">AAAI 2023</span>
                    <span class="venue_complete">Proceedings of the 37th AAAI Conference on Artificial Intelligence (AAAI 2023)</span>
                </div>
                <div class="publication-details  q1 ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <a target="_blank" href="https://github.com/disi-unibo-nlp/cogito-ergo-summ">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/2428/PNG/512/github_black_logo_icon_147128.png"
                                    alt="GitHub Icon">
                                <span class="label">Code</span>
                            </div>
                        </a>
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/conf/aaai/FrisoniISM23.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/view/26503">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    The automatic synthesis of biomedical publications catalyzes a profound research interest elicited by literature congestion. Current sequence-to-sequence models mainly rely on the lexical surface and seldom consider the deep semantic interconnections between the entities mentioned in the source document. Such superficiality translates into fabricated, poorly informative, redundant, and near-extractive summaries that severely restrict their real-world application in biomedicine, where the specialized jargon and the convoluted facts further emphasize task complexity. To fill this gap, we argue that the summarizer should acquire semantic interpretation over input, exploiting structured and unambiguous representations to capture and conserve the most relevant parts of the text content. This paper presents CogitoErgoSumm, the first framework for biomedical abstractive summarization equipping large pre-trained language models with rich semantic graphs. Precisely, we infuse graphs from two complementary semantic parsing techniques with different goals and granularities—Event Extraction and Abstract Meaning Representation, also designing a reward signal to maximize information content preservation through reinforcement learning. Extensive quantitative and qualitative evaluations on the CDSR dataset show that our solution achieves competitive performance according to multiple metrics, despite using 2.5x fewer parameters. Results and ablation studies indicate that our joint text-graph model generates more enlightening, readable, and consistent summaries. Code available at: https://github.com/disi-unibo-nlp/cogito-ergo-summ. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    The automatic synthesis of biomedical publications catalyzes a profound research interest elicited by literature congestion. Current sequence-to-sequence models mainly rely on the lexical surface and seldom consider the deep semantic interconnections between the entities mentioned in the source document. Such superficiality translates into fabricated, poorly informative, redundant, and near-extractive summaries that severely restrict their real-world application in biomedicine, where the specialized jargon and the convoluted facts further emphasize task complexity. To fill this gap, we argue that the summarizer should acquire semantic interpretation over input, exploiting structured and unambiguous representations to capture and conserve the most relevant parts of the text content. This paper presents CogitoErgoSumm, the first framework for biomedical abstractive summarization equipping large pre-trained language models with rich semantic graphs. Precisely, we infuse graphs from two complementary semantic parsing techniques with different goals and granularities—Event Extraction and Abstract Meaning Representation, also designing a reward signal to maximize information content preservation through reinforcement learning. Extensive quantitative and qualitative evaluations on the CDSR dataset show that our solution achieves competitive performance according to multiple metrics, despite using 2.5x fewer parameters. Results and ablation studies indicate that our joint text-graph model generates more enlightening, readable, and consistent summaries. Code available at: https://github.com/disi-unibo-nlp/cogito-ergo-summ. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
        
        
        <!-- Within the same year, sort publications by month (descending order) -->
        

        <!-- LOOP 3: PUBLICATIONS (BY MONTH) -->
        
        
        
        
        
    
        <li class="publication-item 2022 conference" id="111">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">BioReader: a Retrieval-Enhanced Text-to-Text Transformer for Biomedical Literature</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Giacomo Frisoni,</span>
                
                <span class="author">Miki Mizutani,</span>
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Lorenzo Valgimigli</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">EMNLP 2022</span>
                    <span class="venue_complete">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</span>
                </div>
                <div class="publication-details  q1 ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <a target="_blank" href="https://github.com/disi-unibo-nlp/bio-reader">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/2428/PNG/512/github_black_logo_icon_147128.png"
                                    alt="GitHub Icon">
                                <span class="label">Code</span>
                            </div>
                        </a>
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/conf/emnlp/FrisoniMMV22.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://aclanthology.org/2022.emnlp-main.390/">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    The latest batch of research has equipped language models with the ability to attend over relevant and factual information from non-parametric external sources, drawing a complementary path to architectural scaling. Besides mastering language, exploiting and contextualizing the latent world knowledge is crucial in complex domains like biomedicine. However, most works in the field rely on general-purpose models supported by databases like Wikipedia and Books. We introduce BioReader, the first retrieval-enhanced text-to-text model for biomedical natural language processing. Our domain-specific T5-based solution augments the input prompt by fetching and assembling relevant scientific literature chunks from a neural database with ≈60 million tokens centered on PubMed. We fine-tune and evaluate BioReader on a broad array of downstream tasks, significantly outperforming several state-of-the-art methods despite using up to 3x fewer parameters. In tandem with extensive ablation studies, we show that domain knowledge can be easily altered or supplemented to make the model generate correct predictions bypassing the retraining step and thus addressing the literature overload issue. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    The latest batch of research has equipped language models with the ability to attend over relevant and factual information from non-parametric external sources, drawing a complementary path to architectural scaling. Besides mastering language, exploiting and contextualizing the latent world knowledge is crucial in complex domains like biomedicine. However, most works in the field rely on general-purpose models supported by databases like Wikipedia and Books. We introduce BioReader, the first retrieval-enhanced text-to-text model for biomedical natural language processing. Our domain-specific T5-based solution augments the input prompt by fetching and assembling relevant scientific literature chunks from a neural database with ≈60 million tokens centered on PubMed. We fine-tune and evaluate BioReader on a broad array of downstream tasks, significantly outperforming several state-of-the-art methods despite using up to 3x fewer parameters. In tandem with extensive ablation studies, we show that domain knowledge can be easily altered or supplemented to make the model generate correct predictions bypassing the retraining step and thus addressing the literature overload issue. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
        
        
        
        
        
    
        <li class="publication-item 2022 conference" id="333">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Self-supervised Information Retrieval Trained from Self-generated Sets of Queries and Relevant Documents</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Lorenzo Valgimigli,</span>
                
                <span class="author">Alex Rossi,</span>
                
                <span class="author">Cristiano Casadei,</span>
                
                <span class="author">Andrea Montefiori</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">SISAP 2022</span>
                    <span class="venue_complete">Proceedings of the 15th International Conference on Similarity Search and Applications</span>
                </div>
                <div class="publication-details ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/conf/sisap/MoroVRCM22.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://doi.org/10.1007/978-3-031-17849-8_23">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Large corpora of textual data such as scientific papers, patents, legal documents, reviews, etc., represent precious unstructured knowledge that needs semantic information retrieval engines to be extracted. Current best information retrieval solutions use supervised deep learning approaches, requiring large labelled training sets of queries and corresponding relevant documents, often unavailable, or their preparation is economically infeasible for most organizations. In this work, we present a new self-supervised method to train a neural solution to model and efficiently search large corpora of documents against arbitrary queries without requiring labelled dataset of queries and associated relevant papers. The core points of our self-supervised approach are (i) a method to self-generate the training set of queries and their relevant documents from the corpus itself, without any kind of human supervision, (ii) a deep metric learning approach to model their semantic space of relationships, and (iii) the incorporation of a multi-dimensional index for this neural semantic space over which running queries efficiently. To better stress the performance of the approach, we applied it to a totally unsupervised corpus with complex contents of over half a million Italian legal documents. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Large corpora of textual data such as scientific papers, patents, legal documents, reviews, etc., represent precious unstructured knowledge that needs semantic information retrieval engines to be extracted. Current best information retrieval solutions use supervised deep learning approaches, requiring large labelled training sets of queries and corresponding relevant documents, often unavailable, or their preparation is economically infeasible for most organizations. In this work, we present a new self-supervised method to train a neural solution to model and efficiently search large corpora of documents against arbitrary queries without requiring labelled dataset of queries and associated relevant papers. The core points of our self-supervised approach are (i) a method to self-generate the training set of queries and their relevant documents from the corpus itself, without any kind of human supervision, (ii) a deep metric learning approach to model their semantic space of relationships, and (iii) the incorporation of a multi-dimensional index for this neural semantic space over which running queries efficiently. To better stress the performance of the approach, we applied it to a totally unsupervised corpus with complex contents of over half a million Italian legal documents. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
    
        <li class="publication-item 2022 conference" id="444">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Deep Vision-Language Model for Efficient Multi-modal Similarity Search in Fashion Retrieval</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Stefano Salvatori</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">SISAP 2022</span>
                    <span class="venue_complete">Proceedings of the 15th International Conference on Similarity Search and Applications</span>
                </div>
                <div class="publication-details ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/conf/sisap/MoroS22.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://doi.org/10.1007/978-3-031-17849-8_4">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Fashion multi-modal retrieval has been recently addressed using vision-and-language transformers. However, these models cannot scale in training time and memory requirements due to the quadratic attention mechanism. Moreover, they design the retrieval as a classification task, assigning a similarity score to pairs of text and images in input. Each query is thus resolved inefficiently by pairing it, at runtime, with every text or image in the entire dataset, precluding the scalability to large-scale datasets. We propose a novel approach for efficient multi-modal retrieval in the fashion domain that combines self-supervised pretraining with linear attention and deep metric learning to create a latent space where spatial proximity among instances translates into a semantic similarity score. Unlike existing contributions, our approach separately embeds text and images, decoupling them and allowing to collocate and search in the space, after training, even for new images with missing text and vice versa. Experiments show that with a single 12 GB GPU, our solution outperforms, both in efficacy and efficiency, existing state-of-the-art contributions on the FashionGen dataset. Our architecture also enables the adoption of multidimensional indices, with which retrieval scales in logarithmic time up to millions, and potentially billions, of text and images. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Fashion multi-modal retrieval has been recently addressed using vision-and-language transformers. However, these models cannot scale in training time and memory requirements due to the quadratic attention mechanism. Moreover, they design the retrieval as a classification task, assigning a similarity score to pairs of text and images in input. Each query is thus resolved inefficiently by pairing it, at runtime, with every text or image in the entire dataset, precluding the scalability to large-scale datasets. We propose a novel approach for efficient multi-modal retrieval in the fashion domain that combines self-supervised pretraining with linear attention and deep metric learning to create a latent space where spatial proximity among instances translates into a semantic similarity score. Unlike existing contributions, our approach separately embeds text and images, decoupling them and allowing to collocate and search in the space, after training, even for new images with missing text and vice versa. Experiments show that with a single 12 GB GPU, our solution outperforms, both in efficacy and efficiency, existing state-of-the-art contributions on the FashionGen dataset. Our architecture also enables the adoption of multidimensional indices, with which retrieval scales in logarithmic time up to millions, and potentially billions, of text and images. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
    
        <li class="publication-item 2022 conference" id="555">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">NLG-Metricverse: An End-to-End Library for Evaluating Natural Language Generation</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Giacomo Frisoni,</span>
                
                <span class="author">Antonella Carbonaro,</span>
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Andrea Zammarchi,</span>
                
                <span class="author">Marco Avagnano</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">COLING 2022</span>
                    <span class="venue_complete">Proceedings of the 29th International Conference on Computational Linguistics (COLING 2022)</span>
                </div>
                <div class="publication-details ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <a target="_blank" href="https://github.com/disi-unibo-nlp/nlg-metricverse">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/2428/PNG/512/github_black_logo_icon_147128.png"
                                    alt="GitHub Icon">
                                <span class="label">Code</span>
                            </div>
                        </a>
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/conf/coling/FrisoniCMZA22.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://aclanthology.org/2022.coling-1.306">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Driven by deep learning breakthroughs, natural language generation (NLG) models have been at the center of steady progress in the last few years, with a ubiquitous task influence. However, since our ability to generate human-indistinguishable artificial text lags behind our capacity to assess it, it is paramount to develop and apply even better automatic evaluation metrics. To facilitate researchers to judge the effectiveness of their models broadly, we introduce NLG-Metricverse—an end-to-end open-source library for NLG evaluation based on Python. Our framework provides a living collection of NLG metrics in a unified and easy-to-use environment, supplying tools to efficiently apply, analyze, compare, and visualize them. This includes (i) the extensive support to heterogeneous automatic metrics with n-arity management, (ii) the meta-evaluation upon individual performance, metric-metric and metric-human correlations, (iii) graphical interpretations for helping humans better gain score intuitions, (iv) formal categorization and convenient documentation to accelerate metrics understanding. NLG-Metricverse aims to increase the comparability and replicability of NLG research, hopefully stimulating new contributions in the area. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Driven by deep learning breakthroughs, natural language generation (NLG) models have been at the center of steady progress in the last few years, with a ubiquitous task influence. However, since our ability to generate human-indistinguishable artificial text lags behind our capacity to assess it, it is paramount to develop and apply even better automatic evaluation metrics. To facilitate researchers to judge the effectiveness of their models broadly, we introduce NLG-Metricverse—an end-to-end open-source library for NLG evaluation based on Python. Our framework provides a living collection of NLG metrics in a unified and easy-to-use environment, supplying tools to efficiently apply, analyze, compare, and visualize them. This includes (i) the extensive support to heterogeneous automatic metrics with n-arity management, (ii) the meta-evaluation upon individual performance, metric-metric and metric-human correlations, (iii) graphical interpretations for helping humans better gain score intuitions, (iv) formal categorization and convenient documentation to accelerate metrics understanding. NLG-Metricverse aims to increase the comparability and replicability of NLG research, hopefully stimulating new contributions in the area. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
    
        <li class="publication-item 2022 conference" id="666">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Text-to-Text Extraction and Verbalization of Biomedical Event Graphs</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Giacomo Frisoni,</span>
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Lorenzo Balzani</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">COLING 2022</span>
                    <span class="venue_complete">Proceedings of the 29th International Conference on Computational Linguistics (COLING 2022)</span>
                </div>
                <div class="publication-details ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <a target="_blank" href="https://github.com/disi-unibo-nlp/bio-ee-egv">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/2428/PNG/512/github_black_logo_icon_147128.png"
                                    alt="GitHub Icon">
                                <span class="label">Code</span>
                            </div>
                        </a>
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/conf/coling/FrisoniMB22.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://aclanthology.org/2022.coling-1.238/">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Biomedical events represent complex, graphical, and semantically rich interactions expressed in the scientific literature. Almost all contributions in the event realm orbit around semantic parsing, usually employing discriminative architectures and cumbersome multi-step pipelines limited to a small number of target interaction types. We present the first lightweight framework to solve both event extraction and event verbalization with a unified text-to-text approach, allowing us to fuse all the resources so far designed for different tasks. To this end, we present a new event graph linearization technique and release highly comprehensive event-text paired datasets, covering more than 150 event types from multiple biology subareas (English language). By streamlining parsing and generation to translations, we propose baseline transformer model results according to multiple biomedical text mining benchmarks and NLG metrics. Our extractive models achieve greater state-of-the-art performance than single-task competitors and show promising capabilities for the controlled generation of coherent natural language utterances from structured data. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Biomedical events represent complex, graphical, and semantically rich interactions expressed in the scientific literature. Almost all contributions in the event realm orbit around semantic parsing, usually employing discriminative architectures and cumbersome multi-step pipelines limited to a small number of target interaction types. We present the first lightweight framework to solve both event extraction and event verbalization with a unified text-to-text approach, allowing us to fuse all the resources so far designed for different tasks. To this end, we present a new event graph linearization technique and release highly comprehensive event-text paired datasets, covering more than 150 event types from multiple biology subareas (English language). By streamlining parsing and generation to translations, we propose baseline transformer model results according to multiple biomedical text mining benchmarks and NLG metrics. Our extractive models achieve greater state-of-the-art performance than single-task competitors and show promising capabilities for the controlled generation of coherent natural language utterances from structured data. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
    
        <li class="publication-item 2022 conference" id="777">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Enhancing Biomedical Scientific Reviews Summarization with Graph-based Factual Evidence Extracted from Papers</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Giacomo Frisoni,</span>
                
                <span class="author">Paolo Italiani,</span>
                
                <span class="author">Francesco Boschi,</span>
                
                <span class="author">Gianluca Moro</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">DATA 2022</span>
                    <span class="venue_complete">Proceedings of the 11th International Conference on Data Science, Technology and Applications</span>
                </div>
                <div class="publication-details ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/conf/data/FrisoniIBM22.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://www.scitepress.org/PublicationsDetail.aspx?ID=/jornliCVuw=&t=1">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            <span class="award">
                <img class="buttons-icon"
                style="width: 20px;"
                src="https://cdn.icon-icons.com/icons2/2448/PNG/512/trophy_award_icon_148758.png"
                alt="Award Icon">
                Best Student Paper Award
            </span>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Combining structured knowledge and neural language models to tackle natural language processing tasks is a recent research trend that catalyzes community attention. This integration holds a lot of potential in document summarization, especially in the biomedical domain, where the jargon and the complex facts make the overarching information truly hard to interpret. In this context, graph construction via semantic parsing plays a crucial role in unambiguously capturing the most relevant parts of a document. However, current works are limited to extracting open-domain triples, failing to model real-world n-ary and nested biomedical interactions accurately. To alleviate this issue, we present EASumm, the first framework for biomedical abstractive summarization enhanced by event graph extraction (i.e., graphical representations of medical evidence learned from scientific text), relying on dual text-graph encoders. Extensive evaluations on the CDSR dataset corroborate the importance of explicit event structures, with better or comparable performance than previous state-of-the-art systems. Finally, we offer some hints to guide future research in the field. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Combining structured knowledge and neural language models to tackle natural language processing tasks is a recent research trend that catalyzes community attention. This integration holds a lot of potential in document summarization, especially in the biomedical domain, where the jargon and the complex facts make the overarching information truly hard to interpret. In this context, graph construction via semantic parsing plays a crucial role in unambiguously capturing the most relevant parts of a document. However, current works are limited to extracting open-domain triples, failing to model real-world n-ary and nested biomedical interactions accurately. To alleviate this issue, we present EASumm, the first framework for biomedical abstractive summarization enhanced by event graph extraction (i.e., graphical representations of medical evidence learned from scientific text), relying on dual text-graph encoders. Extensive evaluations on the CDSR dataset corroborate the importance of explicit event structures, with better or comparable performance than previous state-of-the-art systems. Finally, we offer some hints to guide future research in the field. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
    
        <li class="publication-item 2022 conference" id="888">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Discriminative Marginalized Probabilistic Neural Method for Multi-Document Summarization of Medical Literature</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Luca Ragazzi,</span>
                
                <span class="author">Lorenzo Valgimigli,</span>
                
                <span class="author">Davide Freddi</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">ACL 2022</span>
                    <span class="venue_complete">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>
                </div>
                <div class="publication-details  q1 ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/conf/acl/MoroRVF22.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://aclanthology.org/2022.acl-long.15/">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Although current state-of-the-art Transformer-based solutions succeeded in a wide range for single-document NLP tasks, they still struggle to address multi-input tasks such as multi-document summarization. Many solutions truncate the inputs, thus ignoring potential summary-relevant contents, which is unacceptable in the medical domain where each information can be vital. Others leverage linear model approximations to apply multi-input concatenation, worsening the results because all information is considered, even if it is conflicting or noisy with respect to a shared background. Despite the importance and social impact of medicine, there are no ad-hoc solutions for multi-document summarization. For this reason, we propose a novel discriminative marginalized probabilistic method (DAMEN) trained to discriminate critical information from a cluster of topic-related medical documents and generate a multi-document summary via token probability marginalization. Results prove we outperform the previous state-of-the-art on a biomedical dataset for multi-document summarization of systematic literature reviews. Moreover, we perform extensive ablation studies to motivate the design choices and prove the importance of each module of our method. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Although current state-of-the-art Transformer-based solutions succeeded in a wide range for single-document NLP tasks, they still struggle to address multi-input tasks such as multi-document summarization. Many solutions truncate the inputs, thus ignoring potential summary-relevant contents, which is unacceptable in the medical domain where each information can be vital. Others leverage linear model approximations to apply multi-input concatenation, worsening the results because all information is considered, even if it is conflicting or noisy with respect to a shared background. Despite the importance and social impact of medicine, there are no ad-hoc solutions for multi-document summarization. For this reason, we propose a novel discriminative marginalized probabilistic method (DAMEN) trained to discriminate critical information from a cluster of topic-related medical documents and generate a multi-document summary via token probability marginalization. Results prove we outperform the previous state-of-the-art on a biomedical dataset for multi-document summarization of systematic literature reviews. Moreover, we perform extensive ablation studies to motivate the design choices and prove the importance of each module of our method. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
    
        <li class="publication-item 2022 conference" id="999">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Semantic Self-Segmentation for Abstractive Summarization of Long Documents in Low-Resource Regimes</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Luca Ragazzi</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">AAAI 2022</span>
                    <span class="venue_complete">Proceedings of the 36th AAAI Conference on Artificial Intelligence (AAAI 2022)</span>
                </div>
                <div class="publication-details  q1 ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <a target="_blank" href="https://disi-unibo-nlp.github.io/publications-site/se3/">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn-icons-png.freepik.com/256/903/903482.png?semt=ais_hybrid"
                                    alt="Webapp Icon">
                                <span class="label">Try!</span>
                            </div>
                        </a>
                        
                        <!-- CODE -->
                        
                        <a target="_blank" href="https://github.com/disi-unibo-nlp/se3">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/2428/PNG/512/github_black_logo_icon_147128.png"
                                    alt="GitHub Icon">
                                <span class="label">Code</span>
                            </div>
                        </a>
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/conf/aaai/MoroR22.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/view/21357">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    The quadratic memory complexity of transformers prevents long document summarization in low computational resource scenarios. State-of-the-art models need to apply input truncation, thus discarding and ignoring potential summary-relevant contents, leading to a performance drop. Furthermore, this loss is generally destructive for semantic text analytics in high-impact domains such as the legal one. In this paper, we propose a novel semantic self-segmentation (Se3) approach for long document summarization to address the critical problems of low-resource regimes, namely to process inputs longer than the GPU memory capacity and produce accurate summaries despite the availability of only a few dozens of training instances. Se3 segments a long input into semantically coherent chunks, allowing transformers to summarize very long documents without truncation by summarizing each chunk and concatenating the results. Experimental outcomes show the approach significantly improves the performance of abstractive summarization transformers, even with just a dozen of labeled data, achieving new state-of-the-art results on two legal datasets of different domains and contents. Finally, we report ablation studies to evaluate each contribution of the components of our method to the performance gain. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    The quadratic memory complexity of transformers prevents long document summarization in low computational resource scenarios. State-of-the-art models need to apply input truncation, thus discarding and ignoring potential summary-relevant contents, leading to a performance drop. Furthermore, this loss is generally destructive for semantic text analytics in high-impact domains such as the legal one. In this paper, we propose a novel semantic self-segmentation (Se3) approach for long document summarization to address the critical problems of low-resource regimes, namely to process inputs longer than the GPU memory capacity and produce accurate summaries despite the availability of only a few dozens of training instances. Se3 segments a long input into semantically coherent chunks, allowing transformers to summarize very long documents without truncation by summarizing each chunk and concatenating the results. Experimental outcomes show the approach significantly improves the performance of abstractive summarization transformers, even with just a dozen of labeled data, achieving new state-of-the-art results on two legal datasets of different domains and contents. Finally, we report ablation studies to evaluate each contribution of the components of our method to the performance gain. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
        
        
        <!-- Within the same year, sort publications by month (descending order) -->
        

        <!-- LOOP 3: PUBLICATIONS (BY MONTH) -->
        
        
        
        
        
        
        
        
        
        
        
        
        <!-- Within the same year, sort publications by month (descending order) -->
        

        <!-- LOOP 3: PUBLICATIONS (BY MONTH) -->
        
        
        
        
        
    
        <li class="publication-item 2020 conference" id="111">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Unsupervised Descriptive Text Mining for Knowledge Graph Learning</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Giacomo Frisoni,</span>
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Antonella Carbonaro</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">KDIR 2020</span>
                    <span class="venue_complete">Proceedings of the 12th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management</span>
                </div>
                <div class="publication-details ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/conf/ic3k/FrisoniMC20.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://www.scitepress.org/Papers/2020/101536/101536.pdf">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    The use of knowledge graphs (KGs) in advanced applications is constantly growing, as a consequence of their ability to model large collections of semantically interconnected data. The extraction of relational facts from plain text is currently one of the main approaches for the construction and expansion of KGs. In this paper, we introduce a novel unsupervised and automatic technique of KG learning from corpora of short unstructured and unlabeled texts. Our approach is unique in that it starts from raw textual data and comes to: i) identify a set of relevant domain-dependent terms; ii) extract aggregate and statistically significant semantic relationships between terms, documents, and classes; iii) represent the accurate probabilistic knowledge as a KG; iv) extend and integrate the KG according to the Linked Open Data vision. The proposed solution is easily transferable to many domains and languages as long as the data are available. As a case study, we demonstrate how it is possible to automatically learn a KG representing the knowledge contained within the conversational messages shared on social networks such as Facebook by patients with rare diseases, and the impact this can have on creating resources aimed to capture the 'voice of patients'. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    The use of knowledge graphs (KGs) in advanced applications is constantly growing, as a consequence of their ability to model large collections of semantically interconnected data. The extraction of relational facts from plain text is currently one of the main approaches for the construction and expansion of KGs. In this paper, we introduce a novel unsupervised and automatic technique of KG learning from corpora of short unstructured and unlabeled texts. Our approach is unique in that it starts from raw textual data and comes to: i) identify a set of relevant domain-dependent terms; ii) extract aggregate and statistically significant semantic relationships between terms, documents, and classes; iii) represent the accurate probabilistic knowledge as a KG; iv) extend and integrate the KG according to the Linked Open Data vision. The proposed solution is easily transferable to many domains and languages as long as the data are available. As a case study, we demonstrate how it is possible to automatically learn a KG representing the knowledge contained within the conversational messages shared on social networks such as Facebook by patients with rare diseases, and the impact this can have on creating resources aimed to capture the 'voice of patients'. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
        
        
        
        
        
    
        <li class="publication-item 2020 conference" id="333">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Learning Interpretable and Statistically Significant Knowledge from Unlabeled Corpora of Social Text Messages: A Novel Methodology of Descriptive Text Mining</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Giacomo Frisoni,</span>
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Antonella Carbonaro</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">DATA 2020</span>
                    <span class="venue_complete">Proceedings of the 9th International Conference on Data Science, Technology and Applications</span>
                </div>
                <div class="publication-details ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/conf/data/FrisoniMC20.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://www.scitepress.org/Papers/2020/98920/98920.pdf">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            <span class="award">
                <img class="buttons-icon"
                style="width: 20px;"
                src="https://cdn.icon-icons.com/icons2/2448/PNG/512/trophy_award_icon_148758.png"
                alt="Award Icon">
                Best Paper Award
            </span>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Though the strong evolution of knowledge learning models has characterized the last few years, the explanation of a phenomenon from text documents, called descriptive text mining, is still a difficult and poorly addressed problem. The need to work with unlabeled data, explainable approaches, unsupervised, and domain-independent solutions further increases the complexity of this task. Currently, existing techniques only partially solve the problem and have several limitations. In this paper, we propose a novel methodology of descriptive text mining, capable of offering accurate explanations in unsupervised settings and of quantifying the results based on their statistical significance. Considering the strong growth of patient communities on social platforms such as Facebook, we demonstrate the effectiveness of the contribution by taking the short social posts related to Esophageal Achalasia as a typical case study. Specifically, the methodology produces useful explanations about the experiences of patients and caregivers. Starting directly from the unlabeled patient's posts, we derive correct scientific correlations among symptoms, drugs, treatments, foods, and so on. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Though the strong evolution of knowledge learning models has characterized the last few years, the explanation of a phenomenon from text documents, called descriptive text mining, is still a difficult and poorly addressed problem. The need to work with unlabeled data, explainable approaches, unsupervised, and domain-independent solutions further increases the complexity of this task. Currently, existing techniques only partially solve the problem and have several limitations. In this paper, we propose a novel methodology of descriptive text mining, capable of offering accurate explanations in unsupervised settings and of quantifying the results based on their statistical significance. Considering the strong growth of patient communities on social platforms such as Facebook, we demonstrate the effectiveness of the contribution by taking the short social posts related to Esophageal Achalasia as a typical case study. Specifically, the methodology produces useful explanations about the experiences of patients and caregivers. Starting directly from the unlabeled patient's posts, we derive correct scientific correlations among symptoms, drugs, treatments, foods, and so on. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
    
        <li class="publication-item 2020 conference" id="444">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Towards Rare Disease Knowledge Graph Learning from Social Posts of Patients</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Giacomo Frisoni,</span>
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Antonella Carbonaro</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">RIIFORUM 2020</span>
                    <span class="venue_complete">Research and Innovation Forum 2020 - Disruptive Technologies in Times of Change</span>
                </div>
                <div class="publication-details ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/conf/riiforum/FrisoniMC20.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-030-62066-0_44">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Rare diseases pose particular challenges to patients, families, caregivers, clinicians and researchers. Due to the scarce availability of information and their disintegration, in recent years we are witnessing a strong growth of patient communities on social platforms such as Facebook. Although the data generated in this context are of high value, the currently existing ontologies and resources tend to ignore them. The work presented in this paper studies how to extract knowledge from the large availability of unstructured text generated by the users over time, in order to represent it in an organized way and to make logical reasoning above. Starting from the awareness of the need to integrate different methodologies in complex domains, the research shows a combined use of Text Mining and Semantic Web techniques. In particular, we describe the basis of a novel approach for Knowledge Graph Learning with the aim of introducing a patient-centered vision into the world of Linked Open Data. By identifying and representing correlations between concepts of interest, we show how it is possible to answer patients’ questions and provide them with an additional tool for decision making. The outlined contribute minimizes costs through automatic data retrieval and increases the productivity of investigators. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Rare diseases pose particular challenges to patients, families, caregivers, clinicians and researchers. Due to the scarce availability of information and their disintegration, in recent years we are witnessing a strong growth of patient communities on social platforms such as Facebook. Although the data generated in this context are of high value, the currently existing ontologies and resources tend to ignore them. The work presented in this paper studies how to extract knowledge from the large availability of unstructured text generated by the users over time, in order to represent it in an organized way and to make logical reasoning above. Starting from the awareness of the need to integrate different methodologies in complex domains, the research shows a combined use of Text Mining and Semantic Web techniques. In particular, we describe the basis of a novel approach for Knowledge Graph Learning with the aim of introducing a patient-centered vision into the world of Linked Open Data. By identifying and representing correlations between concepts of interest, we show how it is possible to answer patients’ questions and provide them with an additional tool for decision making. The outlined contribute minimizes costs through automatic data retrieval and increases the productivity of investigators. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
    </ul>
    
    <h2 class="publication-type-header">
        
        Journal Articles
        
    </h2>

    <ul class="publication-list">

        <!-- LOOP 2: PUBLICATION YEAR -->
        
        <!-- Within the same year, sort publications by month (descending order) -->
        

        <!-- LOOP 3: PUBLICATIONS (BY MONTH) -->
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
    
        <li class="publication-item 2024 journal" id="333">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">LawsuIT: Abstractive Summarization of Constitutional Legal Verdicts</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Luca Ragazzi,</span>
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Stefano Guidi,</span>
                
                <span class="author">Giacomo Frisoni</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">AI&Law 2024</span>
                    <span class="venue_complete">Artificial Intelligence and Law</span>
                </div>
                <div class="publication-details  q1 ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <a target="_blank" href="https://disi-unibo-nlp.github.io/publications-site/lawsuit/">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn-icons-png.freepik.com/256/903/903482.png?semt=ais_hybrid"
                                    alt="Webapp Icon">
                                <span class="label">Try!</span>
                            </div>
                        </a>
                        
                        <!-- CODE -->
                        
                        <!-- CITE -->
                        
                        <!-- READ -->
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Large-scale public datasets are vital for driving the progress of abstractive summarization, especially in law, where documents have highly specialized jargon. However, the available resources are English-centered, limiting research advancements in other languages. This paper introduces LawsuIT, a collection of 14K Italian legal verdicts with expert-authored abstractive maxims drawn from the Constitutional Court of the Italian Republic. LawsuIT presents an arduous task with lengthy source texts and evenly distributed salient content. We offer extensive experiments with sequence-to-sequence and segmentation-based approaches, revealing that the latter achieve better results in full and few-shot settings. We openly release LawsuIT to foster the development and automation of real-world legal applications. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Large-scale public datasets are vital for driving the progress of abstractive summarization, especially in law, where documents have highly specialized jargon. However, the available resources are English-centered, limiting research advancements in other languages. This paper introduces LawsuIT, a collection of 14K Italian legal verdicts with expert-authored abstractive maxims drawn from the Constitutional Court of the Italian Republic. LawsuIT presents an arduous task with lengthy source texts and evenly distributed salient content. We offer extensive experiments with sequence-to-sequence and segmentation-based approaches, revealing that the latter achieve better results in full and few-shot settings. We openly release LawsuIT to foster the development and automation of real-world legal applications. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
        
        
        <!-- Within the same year, sort publications by month (descending order) -->
        

        <!-- LOOP 3: PUBLICATIONS (BY MONTH) -->
        
        
        
        
        
    
        <li class="publication-item 2023 journal" id="111">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Evidence, my Dear Watson: Abstractive Dialogue Summarization on Learnable Relevant Utterances</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Paolo Italiani,</span>
                
                <span class="author">Giacomo Frisoni,</span>
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Antonella Carbonaro,</span>
                
                <span class="author">Claudio Sartori</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">Neurocomputing 2023</span>
                    <span class="venue_complete">Neurocomputing</span>
                </div>
                <div class="publication-details  q1 ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/journals/ijon/ItalianiFMCS24.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0925231223012559">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Abstractive dialogue summarization requires distilling and rephrasing key information from noisy multi-speaker documents. Combining pre-trained language models with input augmentation techniques has recently led to significant research progress. However, existing solutions still struggle to select relevant chat segments, primarily relying on open-domain and unsupervised annotators not tailored to the actual needs of the summarization task. In this paper, we propose DearWatson, a task-aware utterance-level annotation framework for improving the effectiveness and interpretability of pre-trained dialogue summarization models. Precisely, we learn relevant utterances in the source document and mark them with special tags, that then act as supporting evidence for the generated summary. Quantitative experiments are conducted on two datasets made up of real-life messenger conversations. The results show that DearWatson allows model attention to focus on salient tokens, achieving new state-of-the-art results in three evaluation metrics, including semantic and factuality measures. Human evaluation proves the superiority of our solution in semantic consistency and recall. Finally, extensive ablation studies confirm each module’s importance, also exploring different annotation strategies and parameter-efficient fine-tuning of large generative language models. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Abstractive dialogue summarization requires distilling and rephrasing key information from noisy multi-speaker documents. Combining pre-trained language models with input augmentation techniques has recently led to significant research progress. However, existing solutions still struggle to select relevant chat segments, primarily relying on open-domain and unsupervised annotators not tailored to the actual needs of the summarization task. In this paper, we propose DearWatson, a task-aware utterance-level annotation framework for improving the effectiveness and interpretability of pre-trained dialogue summarization models. Precisely, we learn relevant utterances in the source document and mark them with special tags, that then act as supporting evidence for the generated summary. Quantitative experiments are conducted on two datasets made up of real-life messenger conversations. The results show that DearWatson allows model attention to focus on salient tokens, achieving new state-of-the-art results in three evaluation metrics, including semantic and factuality measures. Human evaluation proves the superiority of our solution in semantic consistency and recall. Finally, extensive ablation studies confirm each module’s importance, also exploring different annotation strategies and parameter-efficient fine-tuning of large generative language models. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
        
        
        
        
        
    
        <li class="publication-item 2023 journal" id="333">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Multi-Language Transfer Learning for Low-Resource Legal Case Summarization</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Nicola Piscaglia,</span>
                
                <span class="author">Luca Ragazzi,</span>
                
                <span class="author">Paolo Italiani</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">AI&Law 2023</span>
                    <span class="venue_complete">Artificial Intelligence and Law</span>
                </div>
                <div class="publication-details  q1 ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://www.semanticscholar.org/search?q=Multi-Language%20Transfer%20Learning%20for%20Low-Resource%20Legal%20Case%20Summarization&sort=relevance">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://link.springer.com/article/10.1007/s10506-023-09373-8">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Analyzing and evaluating legal case reports are labor-intensive tasks for judges and lawyers, who usually base their decisions on report abstracts, legal principles, and commonsense reasoning. Thus, summarizing legal documents is time-consuming and requires excellent human expertise. Moreover, public legal corpora of specific languages are almost unavailable. This paper proposes a transfer learning approach with extractive and abstractive techniques to cope with the lack of labeled legal summarization datasets, namely a low-resource scenario. In particular, we conducted extensive multi- and cross-language experiments. The proposed work outperforms the state-of-the-art results of extractive summarization on the Australian Legal Case Reports dataset and sets a new baseline for abstractive summarization. Finally, syntactic and semantic metrics assessments have been carried out to evaluate the accuracy and the factual consistency of the machine-generated legal summaries. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Analyzing and evaluating legal case reports are labor-intensive tasks for judges and lawyers, who usually base their decisions on report abstracts, legal principles, and commonsense reasoning. Thus, summarizing legal documents is time-consuming and requires excellent human expertise. Moreover, public legal corpora of specific languages are almost unavailable. This paper proposes a transfer learning approach with extractive and abstractive techniques to cope with the lack of labeled legal summarization datasets, namely a low-resource scenario. In particular, we conducted extensive multi- and cross-language experiments. The proposed work outperforms the state-of-the-art results of extractive summarization on the Australian Legal Case Reports dataset and sets a new baseline for abstractive summarization. Finally, syntactic and semantic metrics assessments have been carried out to evaluate the accuracy and the factual consistency of the machine-generated legal summaries. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
        
        
        
        
        
    
        <li class="publication-item 2023 journal" id="555">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Align-Then-Abstract Representation Learning for Low-Resource Summarization</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Luca Ragazzi</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">Neurocomputing 2023</span>
                    <span class="venue_complete">Neurocomputing</span>
                </div>
                <div class="publication-details  q1 ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/journals/ijon/MoroR23.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://doi.org/10.1016/j.neucom.2023.126356">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Generative transformer-based models have achieved state-of-the-art performance in text summarization. Nevertheless, they still struggle in real-world scenarios with long documents when trained in low-resource settings of a few dozen labeled training instances, namely in low-resource summarization (LRS). This paper bridges the gap by addressing two key research challenges when summarizing long documents, i.e., long-input processing and document representation, in one coherent model trained for LRS. Specifically, our novel align-then-abstract representation learning model (Athena) jointly trains a segmenter and a summarizer by maximizing the alignment between the chunk-target pairs in output from the text segmentation. Extensive experiments reveal that Athena outperforms the current state-of-the-art approaches in LRS on multiple long document summarization datasets from different domains. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Generative transformer-based models have achieved state-of-the-art performance in text summarization. Nevertheless, they still struggle in real-world scenarios with long documents when trained in low-resource settings of a few dozen labeled training instances, namely in low-resource summarization (LRS). This paper bridges the gap by addressing two key research challenges when summarizing long documents, i.e., long-input processing and document representation, in one coherent model trained for LRS. Specifically, our novel align-then-abstract representation learning model (Athena) jointly trains a segmenter and a summarizer by maximizing the alignment between the chunk-target pairs in output from the text segmentation. Extensive experiments reveal that Athena outperforms the current state-of-the-art approaches in LRS on multiple long document summarization datasets from different domains. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
    
        <li class="publication-item 2023 journal" id="666">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Efficient Memory-Enhanced Transformer for Long-Document Summarization in Low-Resource Regimes</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Luca Ragazzi,</span>
                
                <span class="author">Lorenzo Valgimigli,</span>
                
                <span class="author">Giacomo Frisoni,</span>
                
                <span class="author">Claudio Sartori,</span>
                
                <span class="author">Gustavo Marfia</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">Sensors 2023</span>
                    <span class="venue_complete">Sensors</span>
                </div>
                <div class="publication-details  q1 ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/journals/sensors/MoroRVFSM23.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://www.mdpi.com/1424-8220/23/7/3542">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Long document summarization poses obstacles to current generative transformer-based models because of the broad context to process and understand. Indeed, detecting long-range dependencies is still challenging for today’s state-of-the-art solutions, usually requiring model expansion at the cost of an unsustainable demand for computing and memory capacities. This paper introduces Emma, a novel efficient memory-enhanced transformer-based architecture. By segmenting a lengthy input into multiple text fragments, our model stores and compares the current chunk with previous ones, gaining the capability to read and comprehend the entire context over the whole document with a fixed amount of GPU memory. This method enables the model to deal with theoretically infinitely long documents, using less than 18 and 13 GB of memory for training and inference, respectively. We conducted extensive performance analyses and demonstrate that Emma achieved competitive results on two datasets of different domains while consuming significantly less GPU memory than competitors do, even in low-resource settings. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Long document summarization poses obstacles to current generative transformer-based models because of the broad context to process and understand. Indeed, detecting long-range dependencies is still challenging for today’s state-of-the-art solutions, usually requiring model expansion at the cost of an unsustainable demand for computing and memory capacities. This paper introduces Emma, a novel efficient memory-enhanced transformer-based architecture. By segmenting a lengthy input into multiple text fragments, our model stores and compares the current chunk with previous ones, gaining the capability to read and comprehend the entire context over the whole document with a fixed amount of GPU memory. This method enables the model to deal with theoretically infinitely long documents, using less than 18 and 13 GB of memory for training and inference, respectively. We conducted extensive performance analyses and demonstrate that Emma achieved competitive results on two datasets of different domains while consuming significantly less GPU memory than competitors do, even in low-resource settings. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
    
        <li class="publication-item 2023 journal" id="777">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Efficient Text-Image Semantic Search: a Multi-modal Vision-Language Approach for Fashion Retrieval</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Stefano Salvatori,</span>
                
                <span class="author">Giacomo Frisoni</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">Neurocomputing 2023</span>
                    <span class="venue_complete">Neurocomputing</span>
                </div>
                <div class="publication-details  q1 ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <a target="_blank" href="https://disi-unibo-nlp.github.io/publications-site/fashion_retrieval/">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn-icons-png.freepik.com/256/903/903482.png?semt=ais_hybrid"
                                    alt="Webapp Icon">
                                <span class="label">Try!</span>
                            </div>
                        </a>
                        
                        <!-- CODE -->
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/journals/ijon/MoroSF23.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://doi.org/10.1016/j.neucom.2023.03.057">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    In this paper, we address the problem of multi-modal retrieval of fashion products. State-of-the-art (SOTA) works proposed in literature use vision-and-language transformers to assign similarity scores to joint text-image pairs, then used for sorting the results during a retrieval phase. However, this approach is inefficient since it requires coupling a query with every record in the dataset and computing a forward pass for each sample at runtime, precluding scalability to large-scale datasets. We thus propose a solution that overcomes the above limitation by combining transformers and deep metric learning to create a latent space where texts and images are separately embedded, and their spatial proximity translates into semantic similarity. Our architecture does not use convolutional neural networks to process images, allowing us to test different levels of image-processing details and metric learning losses. We vastly improve retrieval accuracy results on the FashionGen benchmark (+18.71% and +9.22% Rank@1 on Image-to-Text and Text-to-Image, respectively) while being up to 512x faster. Finally, we analyze the speed-up obtainable by different approximate nearest neighbor retrieval strategies—an optimization precluded to current SOTA contributions. We release our solution as a web application available at https://disi-unibo-nlp.github.io/projects/fashion_retrieval/. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    In this paper, we address the problem of multi-modal retrieval of fashion products. State-of-the-art (SOTA) works proposed in literature use vision-and-language transformers to assign similarity scores to joint text-image pairs, then used for sorting the results during a retrieval phase. However, this approach is inefficient since it requires coupling a query with every record in the dataset and computing a forward pass for each sample at runtime, precluding scalability to large-scale datasets. We thus propose a solution that overcomes the above limitation by combining transformers and deep metric learning to create a latent space where texts and images are separately embedded, and their spatial proximity translates into semantic similarity. Our architecture does not use convolutional neural networks to process images, allowing us to test different levels of image-processing details and metric learning losses. We vastly improve retrieval accuracy results on the FashionGen benchmark (+18.71% and +9.22% Rank@1 on Image-to-Text and Text-to-Image, respectively) while being up to 512x faster. Finally, we analyze the speed-up obtainable by different approximate nearest neighbor retrieval strategies—an optimization precluded to current SOTA contributions. We release our solution as a web application available at https://disi-unibo-nlp.github.io/projects/fashion_retrieval/. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
    
        <li class="publication-item 2023 journal" id="101010">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Graph-Enhanced Biomedical Abstractive Summarization via Factual Evidence Extraction</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Giacomo Frisoni,</span>
                
                <span class="author">Paolo Italiani,</span>
                
                <span class="author">Gianluca Moro</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">DATA (Revised Selected Papers) 2023</span>
                    <span class="venue_complete">SN Computer Science</span>
                </div>
                <div class="publication-details ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <a target="_blank" href="https://github.com/disi-unibo-nlp/easumm">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/2428/PNG/512/github_black_logo_icon_147128.png"
                                    alt="GitHub Icon">
                                <span class="label">Code</span>
                            </div>
                        </a>
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/journals/sncs/FrisoniIMBBC23.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Infusing structured semantic representations into language models is a rising research trend underpinning many natural language processing tasks that require understanding and reasoning capabilities. Decoupling factual non-ambiguous concept units from the lexical surface holds great potential in abstractive summarization, especially in the biomedical domain, where fact selection and rephrasing are made more difficult by specialized jargon and hard factuality constraints. Nevertheless, current graph-augmented contributions rely on extractive binary relations, failing to model real-world n-ary and nested biomedical interactions mentioned in the text. To alleviate this issue, we present EASumm, the first framework for biomedical abstractive summarization empowered by event extraction, namely graph-based representations of relevant medical evidence derived from the source scientific document. By relying on dual text-graph encoders, we prove the promising role of explicit event structures, achieving better or comparable performance than previous state-of-the-art models on the CDSR dataset. We conduct extensive ablation studies, including a wide experimentation of graph representation learning techniques. Finally, we offer some hints to guide future research in the field. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Infusing structured semantic representations into language models is a rising research trend underpinning many natural language processing tasks that require understanding and reasoning capabilities. Decoupling factual non-ambiguous concept units from the lexical surface holds great potential in abstractive summarization, especially in the biomedical domain, where fact selection and rephrasing are made more difficult by specialized jargon and hard factuality constraints. Nevertheless, current graph-augmented contributions rely on extractive binary relations, failing to model real-world n-ary and nested biomedical interactions mentioned in the text. To alleviate this issue, we present EASumm, the first framework for biomedical abstractive summarization empowered by event extraction, namely graph-based representations of relevant medical evidence derived from the source scientific document. By relying on dual text-graph encoders, we prove the promising role of explicit event structures, achieving better or comparable performance than previous state-of-the-art models on the CDSR dataset. We conduct extensive ablation studies, including a wide experimentation of graph representation learning techniques. Finally, we offer some hints to guide future research in the field. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        <!-- Within the same year, sort publications by month (descending order) -->
        

        <!-- LOOP 3: PUBLICATIONS (BY MONTH) -->
        
        
        
        
        
        
        
        
        
        
    
        <li class="publication-item 2022 journal" id="222">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Comprehensive Analysis of Knowledge Graph Embedding Techniques Benchmarked on Link Prediction</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Ilaria Ferrari,</span>
                
                <span class="author">Giacomo Frisoni,</span>
                
                <span class="author">Paolo Italiani,</span>
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Claudio Sartori</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">Electronics 2022</span>
                    <span class="venue_complete">Electronics</span>
                </div>
                <div class="publication-details ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <a target="_blank" href="https://github.com/disi-unibo-nlp/kg-emb-link-pred">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/2428/PNG/512/github_black_logo_icon_147128.png"
                                    alt="GitHub Icon">
                                <span class="label">Code</span>
                            </div>
                        </a>
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://www.semanticscholar.org/paper/Comprehensive-Analysis-of-Knowledge-Graph-Embedding-Ferrari-Frisoni/b5167990eda7d48f1a70a1fcb900ed5d46c40985">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://www.mdpi.com/2079-9292/11/23/3866">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    In knowledge graph representation learning, link prediction is among the most popular and influential tasks. Its surge in popularity has resulted in a panoply of orthogonal embedding-based methods projecting entities and relations into low-dimensional continuous vectors. To further enrich the research space, the community witnessed a prolific development of evaluation benchmarks with a variety of structures and domains. Therefore, researchers and practitioners face an unprecedented challenge in effectively identifying the best solution to their needs. To this end, we propose the most comprehensive and up-to-date study to systematically assess the effectiveness and efficiency of embedding models for knowledge graph completion. We compare 13 models on six datasets with different sizes, domains, and relational properties, covering translational, semantic matching, and neural network-based encoders. A fine-grained evaluation is conducted to compare each technique head-to-head in terms of standard metrics, training and evaluation times, memory consumption, carbon footprint, and space geometry. Our results demonstrate the high dependence between performance and graph types, identifying the best options for each scenario. Among all the encoding strategies, the new generation of translational models emerges as the most promising, bringing out the best and most consistent results across all the datasets and evaluation criteria. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    In knowledge graph representation learning, link prediction is among the most popular and influential tasks. Its surge in popularity has resulted in a panoply of orthogonal embedding-based methods projecting entities and relations into low-dimensional continuous vectors. To further enrich the research space, the community witnessed a prolific development of evaluation benchmarks with a variety of structures and domains. Therefore, researchers and practitioners face an unprecedented challenge in effectively identifying the best solution to their needs. To this end, we propose the most comprehensive and up-to-date study to systematically assess the effectiveness and efficiency of embedding models for knowledge graph completion. We compare 13 models on six datasets with different sizes, domains, and relational properties, covering translational, semantic matching, and neural network-based encoders. A fine-grained evaluation is conducted to compare each technique head-to-head in terms of standard metrics, training and evaluation times, memory consumption, carbon footprint, and space geometry. Our results demonstrate the high dependence between performance and graph types, identifying the best options for each scenario. Among all the encoding strategies, the new generation of translational models emerges as the most promising, bringing out the best and most consistent results across all the datasets and evaluation criteria. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
    
        <li class="publication-item 2022 journal" id="101010">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Human Being Detection from UWB NLOS Signals: Accuracy and Generality of Advanced Machine Learning Models</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Federico Di Luca,</span>
                
                <span class="author">Davide Dardari,</span>
                
                <span class="author">Giacomo Frisoni</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">Sensors 2022</span>
                    <span class="venue_complete">Sensors</span>
                </div>
                <div class="publication-details  q1 ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <a target="_blank" href="https://github.com/disi-unibo-nlp/uwb-nlos-human-detection">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/2428/PNG/512/github_black_logo_icon_147128.png"
                                    alt="GitHub Icon">
                                <span class="label">Code</span>
                            </div>
                        </a>
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/journals/sensors/MoroLDF22.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://www.mdpi.com/1424-8220/22/4/1656">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    This paper studies the problem of detecting human beings in non-line-of-sight (NLOS) conditions using an ultra-wideband radar. We perform an extensive measurement campaign in realistic environments, considering different body orientations, the obstacles’ materials, and radar–obstacle distances. We examine two main scenarios according to the radar position: (i) placed on top of a mobile cart; (ii) handheld at different heights. We empirically analyze and compare several input representations and machine learning (ML) methods—supervised and unsupervised, symbolic and non-symbolic—according to both their accuracy in detecting NLOS human beings and their adaptability to unseen cases. Our study proves the effectiveness and flexibility of modern ML techniques, avoiding environment-specific configurations and benefiting from knowledge transference. Unlike traditional TLC approaches, ML allows for generalization, overcoming limits due to unknown or only partially known observation models and insufficient labeled data, which usually occur in emergencies or in the presence of time/cost constraints. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    This paper studies the problem of detecting human beings in non-line-of-sight (NLOS) conditions using an ultra-wideband radar. We perform an extensive measurement campaign in realistic environments, considering different body orientations, the obstacles’ materials, and radar–obstacle distances. We examine two main scenarios according to the radar position: (i) placed on top of a mobile cart; (ii) handheld at different heights. We empirically analyze and compare several input representations and machine learning (ML) methods—supervised and unsupervised, symbolic and non-symbolic—according to both their accuracy in detecting NLOS human beings and their adaptability to unseen cases. Our study proves the effectiveness and flexibility of modern ML techniques, avoiding environment-specific configurations and benefiting from knowledge transference. Unlike traditional TLC approaches, ML allows for generalization, overcoming limits due to unknown or only partially known observation models and insufficient labeled data, which usually occur in emergencies or in the presence of time/cost constraints. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        <!-- Within the same year, sort publications by month (descending order) -->
        

        <!-- LOOP 3: PUBLICATIONS (BY MONTH) -->
        
        
        
        
        
    
        <li class="publication-item 2021 journal" id="111">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Unsupervised Event Graph Representation and Similarity Learning on Biomedical Literature</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Giacomo Frisoni,</span>
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Giulio Carlassare,</span>
                
                <span class="author">Antonella Carbonaro</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">Sensors 2021</span>
                    <span class="venue_complete">Sensors</span>
                </div>
                <div class="publication-details  q1 ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <a target="_blank" href="https://github.com/disi-unibo-nlp/ddegk">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/2428/PNG/512/github_black_logo_icon_147128.png"
                                    alt="GitHub Icon">
                                <span class="label">Code</span>
                            </div>
                        </a>
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/journals/sensors/FrisoniMCC22.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://www.mdpi.com/1424-8220/22/1/3">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    The automatic extraction of biomedical events from the scientific literature has drawn keen interest in the last several years, recognizing complex and semantically rich graphical interactions otherwise buried in texts. However, very few works revolve around learning embeddings or similarity metrics for event graphs. This gap leaves biological relations unlinked and prevents the application of machine learning techniques to promote discoveries. Taking advantage of recent deep graph kernel solutions and pre-trained language models, we propose Deep Divergence Event Graph Kernels (DDEGK), an unsupervised inductive method to map events into low-dimensional vectors, preserving their structural and semantic similarities. Unlike most other systems, DDEGK operates at a graph level and does not require task-specific labels, feature engineering, or known correspondences between nodes. To this end, our solution compares events against a small set of anchor ones, trains cross-graph attention networks for drawing pairwise alignments (bolstering interpretability), and employs transformer-based models to encode continuous attributes. Extensive experiments have been done on nine biomedical datasets. We show that our learned event representations can be effectively employed in tasks such as graph classification, clustering, and visualization, also facilitating downstream semantic textual similarity. Empirical results demonstrate that DDEGK significantly outperforms other state-of-the-art methods. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    The automatic extraction of biomedical events from the scientific literature has drawn keen interest in the last several years, recognizing complex and semantically rich graphical interactions otherwise buried in texts. However, very few works revolve around learning embeddings or similarity metrics for event graphs. This gap leaves biological relations unlinked and prevents the application of machine learning techniques to promote discoveries. Taking advantage of recent deep graph kernel solutions and pre-trained language models, we propose Deep Divergence Event Graph Kernels (DDEGK), an unsupervised inductive method to map events into low-dimensional vectors, preserving their structural and semantic similarities. Unlike most other systems, DDEGK operates at a graph level and does not require task-specific labels, feature engineering, or known correspondences between nodes. To this end, our solution compares events against a small set of anchor ones, trains cross-graph attention networks for drawing pairwise alignments (bolstering interpretability), and employs transformer-based models to encode continuous attributes. Extensive experiments have been done on nine biomedical datasets. We show that our learned event representations can be effectively employed in tasks such as graph classification, clustering, and visualization, also facilitating downstream semantic textual similarity. Empirical results demonstrate that DDEGK significantly outperforms other state-of-the-art methods. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
    
        <li class="publication-item 2021 journal" id="222">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">A Survey on Event Extraction for Natural Language Understanding: Riding the Biomedical Literature Wave</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Giacomo Frisoni,</span>
                
                <span class="author">Gianluca Moro,</span>
                
                <span class="author">Antonella Carbonaro</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">IEEE Access 2021</span>
                    <span class="venue_complete">IEEE Access</span>
                </div>
                <div class="publication-details  q1 ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/journals/access/FrisoniMC21.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://ieeexplore.ieee.org/document/9627684">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Motivation: The scientific literature embeds an enormous amount of relational knowledge, encompassing interactions between biomedical entities, like proteins, drugs, and symptoms. To cope with the ever-increasing number of publications, researchers are experiencing a surge of interest in extracting valuable, structured, concise, and unambiguous information from plain texts. With the development of deep learning, the granularity of information extraction is evolving from entities and pairwise relations to events. Events can model complex interactions involving multiple participants having a specific semantic role, also handling nested and overlapping definitions. After being studied for years, automatic event extraction is on the road to significantly impact biology in a wide range of applications, from knowledge base enrichment to the formulation of new research hypotheses. Results: This paper provides a comprehensive and up-to-date survey on the link between event extraction and natural language understanding, focusing on the biomedical domain. First, we establish a flexible event definition, summarizing the terminological efforts conducted in various areas. Second, we present the event extraction task, the related challenges, and the available annotated corpora. Third, we deeply explore the most representative methods and present an analysis of the current state-of-the-art, accompanied by performance discussion. To help researchers navigate the avalanche of event extraction works, we provide a detailed taxonomy for classifying the contributions proposed by the community. Fourth, we compare solutions applied in biomedicine with those evaluated in other domains, identifying research opportunities and providing insights for strategies not yet explored. Finally, we discuss applications and our envisions about future perspectives, moving the needle on explainability and knowledge injection. <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Motivation: The scientific literature embeds an enormous amount of relational knowledge, encompassing interactions between biomedical entities, like proteins, drugs, and symptoms. To cope with the ever-increasing number of publications, researchers are experiencing a surge of interest in extracting valuable, structured, concise, and unambiguous information from plain texts. With the development of deep learning, the granularity of information extraction is evolving from entities and pairwise relations to events. Events can model complex interactions involving multiple participants having a specific semantic role, also handling nested and overlapping definitions. After being studied for years, automatic event extraction is on the road to significantly impact biology in a wide range of applications, from knowledge base enrichment to the formulation of new research hypotheses. Results: This paper provides a comprehensive and up-to-date survey on the link between event extraction and natural language understanding, focusing on the biomedical domain. First, we establish a flexible event definition, summarizing the terminological efforts conducted in various areas. Second, we present the event extraction task, the related challenges, and the available annotated corpora. Third, we deeply explore the most representative methods and present an analysis of the current state-of-the-art, accompanied by performance discussion. To help researchers navigate the avalanche of event extraction works, we provide a detailed taxonomy for classifying the contributions proposed by the community. Fourth, we compare solutions applied in biomedicine with those evaluated in other domains, identifying research opportunities and providing insights for strategies not yet explored. Finally, we discuss applications and our envisions about future perspectives, moving the needle on explainability and knowledge injection. <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        <!-- Within the same year, sort publications by month (descending order) -->
        

        <!-- LOOP 3: PUBLICATIONS (BY MONTH) -->
        
        
        
        
        
        
        
        
        
        
    
        <li class="publication-item 2020 journal" id="222">
            <!-- PUBLICATION TITLE -->
            <h1 class="publication-title">Phenomena Explanation from Text: Unsupervised Learning of Interpretable and Statistically Significant Knowledge</h1>
            <!-- PUBLICATION AUTHORS -->
            <div class="authors">
                
                
                <span class="author">Giacomo Frisoni,</span>
                
                <span class="author">Gianluca Moro</span>
                
            </div>
            <!-- PUBLICATION BADGE - VENUE - BUTTONS -->
            <div class="venue-container">
                <div class="venue-info">
                    <span class="venue-year">DATA (Revised Selected Papers) 2020</span>
                    <span class="venue_complete">Proceedings of the 9th International Conference on Data Management Technologies and Applications</span>
                </div>
                <div class="publication-details ">
                    <div class="buttons-div">
                        <!-- WEBAPP -->
                        
                        <!-- CODE -->
                        
                        <a target="_blank" href="https://github.com/disi-unibo-nlp/POIROT">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/2428/PNG/512/github_black_logo_icon_147128.png"
                                    alt="GitHub Icon">
                                <span class="label">Code</span>
                            </div>
                        </a>
                        
                        <!-- CITE -->
                        
                        <a target="_blank" href="https://dblp.uni-trier.de/rec/conf/data/FrisoniM20.html?view=bibtex">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.iconscout.com/icon/free/png-256/free-dblp-3521374-2944818.png"
                                    alt="Cite Icon">
                                <span class="label">Cite</span>
                            </div>
                        </a>
                        
                        <!-- READ -->
                        
                        <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-030-83014-4_14">
                            <div class="document-icon-container">
                                <img class="buttons-icon"
                                    src="https://cdn.icon-icons.com/icons2/292/PNG/512/PDF_30915.png"
                                    alt="Document Icon">
                                <span class="label">Read</span>
                            </div>
                        </a>
                        
                    </div>
                </div>
            </div>
            
            
            <div class="abstract-content">
                <p class="short-text">
                    Learning knowledge from text is becoming increasingly important as the amount of unstructured content on the Web rapidly grows. Despite recent breakthroughs in natural language understanding, the explanation of phenomena from textual documents is still a difficult and poorly addressed problem. Additionally, current NLP solutions often require labeled data, are domain-dependent, and based on black box models. In this paper, we introduce POIROT, a new descriptive text mining methodology for phenomena explanation from documents corpora. POIROT is designed to provide accurate and interpretable results in unsupervised settings, quantifying them based on their statistical significance. We evaluated POIROT on a medical case study, with the aim of learning the “voice of patients” from short social posts. Taking Esophageal Achalasia as a reference, we automatically derived scientific correlations with 79% F1-measure score and built useful explanations of the patients’ viewpoint on topics such as symptoms, treatments, drugs, and foods. We make the source code and experiment details publicly available (https://github.com/unibodatascience/POIROT). <!-- Full abstract -->
                    <a href="#" class="read-more">More</a>
                </p>
                <p class="full-text" style="display: none;">
                    Learning knowledge from text is becoming increasingly important as the amount of unstructured content on the Web rapidly grows. Despite recent breakthroughs in natural language understanding, the explanation of phenomena from textual documents is still a difficult and poorly addressed problem. Additionally, current NLP solutions often require labeled data, are domain-dependent, and based on black box models. In this paper, we introduce POIROT, a new descriptive text mining methodology for phenomena explanation from documents corpora. POIROT is designed to provide accurate and interpretable results in unsupervised settings, quantifying them based on their statistical significance. We evaluated POIROT on a medical case study, with the aim of learning the “voice of patients” from short social posts. Taking Esophageal Achalasia as a reference, we automatically derived scientific correlations with 79% F1-measure score and built useful explanations of the patients’ viewpoint on topics such as symptoms, treatments, drugs, and foods. We make the source code and experiment details publicly available (https://github.com/unibodatascience/POIROT). <!-- Full abstract -->
                    <a href="#" class="read-less">Less</a>
                </p>
            </div>
            
        </li>
        
        
        
        
        
        
        
        
        
        
        
        
        
    </ul>
    
</div>

</div>

<script>

    function updatePublicationCounter() {
        const publicationItems = document.querySelectorAll('.publication-item:not([style*="display: none"])');
        const totalCount = publicationItems.length;

        // Update the publication counter with the total count
        updatePublicationCounterDisplay(totalCount);
    }

    function updatePublicationCounterDisplay(count) {
        document.getElementById('publication-count').textContent = count;
    }

    document.addEventListener("DOMContentLoaded", function () {
        const readMoreLinks = document.querySelectorAll('.read-more');
        const readLessLinks = document.querySelectorAll('.read-less');

        readMoreLinks.forEach(link => {
            link.addEventListener('click', function (event) {
                event.preventDefault(); // Prevent the default behavior of the anchor link
                const abstractContent = this.closest('.abstract-content');
                abstractContent.querySelector('.short-text').style.display = 'none';
                abstractContent.querySelector('.full-text').style.display = 'block';
            });
        });

        readLessLinks.forEach(link => {
            link.addEventListener('click', function (event) {
                event.preventDefault(); // Prevent the default behavior of the anchor link
                const abstractContent = this.closest('.abstract-content');
                const shortText = abstractContent.querySelector('.short-text');
                const fullText = abstractContent.querySelector('.full-text');

                fullText.style.display = 'none';
                shortText.style.display = 'block';

                // Apply the truncation style to the short-text paragraph
                shortText.style.display = '-webkit-box';
                shortText.style.webkitBoxOrient = 'vertical';
                shortText.style.webkitLineClamp = '1';
                shortText.style.overflow = 'hidden';
            });
        });

    });

    document.addEventListener("DOMContentLoaded", function () {

        const publicationItems = document.querySelectorAll('.publication-item');

        const filterButtonsYear = document.querySelectorAll(".filter-buttons button[data-year]");
        const filterButtonsType = document.querySelectorAll(".filter-buttons-type button[data-type]");
        const yearDropdown = document.getElementById("year-dropdown");
        const typeDropdown = document.getElementById("type-dropdown");
        
        const awardCheckbox = document.getElementById("award-checkbox");
        const q1Checkbox = document.getElementById("q1-checkbox");

        updatePublicationCounter();

        // Event listener for dropdowns change
        yearDropdown.addEventListener("change", handleDropdownChange);
        typeDropdown.addEventListener("change", handleDropdownChange);

        // Function to reset the search bar
        function resetSearchBar() {
            const publicationSearch = document.getElementById("publication-search");
            publicationSearch.value = ''; // Clear the search input value
        }
        
        function handleDropdownChange() {
            resetSearchBar();
            const year = yearDropdown.options[yearDropdown.selectedIndex].getAttribute("data-year");
            const type = typeDropdown.options[typeDropdown.selectedIndex].getAttribute("data-type");
            filterPublications(year, type);
        }

        filterButtonsYear.forEach(button => {
            button.addEventListener("click", function () {
                resetSearchBar();
                const year = this.getAttribute("data-year");
                const type = document.querySelector(".filter-buttons-type .active")
                    .getAttribute("data-type");
                filterPublications(year, type);
                // Update button styling to indicate active state
                filterButtonsYear.forEach(btn => {
                    btn.classList.remove("active");
                });
                this.classList.add("active");
            });
        });

        filterButtonsType.forEach(button => {
            button.addEventListener("click", function () {
                resetSearchBar();
                const type = this.getAttribute("data-type");
                const year = document.querySelector(".filter-buttons .active").getAttribute(
                    "data-year");
                filterPublications(year, type);
                // Update button styling to indicate active state
                filterButtonsType.forEach(btn => {
                    btn.classList.remove("active");
                });
                this.classList.add("active");
            });
        });

        function filterPublications(year, type) {
            const publicationItems = document.querySelectorAll(".publication-item");
            let hasPublicationsForYear = false; 
            let showHeader = false; // Variable to track if the header should be shown
            let hasPublicationsForYearAndType = false;
            let showHighlights = true;
            let count = 0;

            publicationItems.forEach(item => {
                // Splitting the classes into an array
                const classes = Array.from(item.classList);
                let typeItem = classes[classes.length - 1];
                let yearItem = classes[classes.length - 2];

                if ((year === "all" || yearItem === year) && (type === "all-type" || typeItem === type)) {
                    hasPublicationsForYearAndType = true;
                    if (year === "all" || type === "all-type") {
                        hasPublicationsForYear = true;
                    }
                    if (type !== "all-type" || year !== "all") {
                        showHighlights = false;
                    }
                    if (item.classList.contains('highlights')) {
                        if (showHighlights) {
                            item.style.display = "block";
                        } else {
                            item.style.display = "none";
                        }
                    } else {
                        item.style.display = "block"; // Show publication item
                        count++;
                    }

                } else {
                    item.style.display = "none"; // Hide publication item
                }
                updatePublicationCounter(count);
            });

            // Show or hide the type header based on whether there are publications for each type when type is "all-type"
            const headers = document.querySelectorAll(".publication-type-header");
            if (type === "all-type") {
                if (year === "all" || hasPublicationsForYear) {
                    headers.forEach(header => {
                        // Check if there are any publications for the corresponding type
                        const typeClass = header.textContent.trim().toLowerCase();
                        let typeClassValue = "";
                        if (typeClass == "conference proceedings") {
                            typeClassValue = "conference";
                        } else {
                            typeClassValue = "journal";
                        }
                        const typePublications = document.querySelectorAll(
                            `.publication-item.${typeClassValue}`);
                        const hasPublicationsForType = Array.from(typePublications).some(item => item
                            .style.display !== "none");

                        if (hasPublicationsForType) {
                            header.style.display = "block";
                        } else {
                            header.style.display = "none";
                        }
                    });
                } else {
                    headers.forEach(header => {
                        header.style.display = "none";
                    });
                }
            } else {
                headers.forEach(header => {
                    header.style.display = "none";
                });
            }

        } 

        document.getElementById('award-checkbox').addEventListener('change', function() {
                let count = 0;
                var showOnlyAwarded = document.getElementById('award-checkbox').checked;
                var publications = document.querySelectorAll('.publication-item');
                const awardCheckbox = this;
                
                const allButtons = document.querySelectorAll('.filter-button');

                const headers = document.querySelectorAll('.publication-type-header');

                // Disable all buttons initially
                allButtons.forEach(button => {
                    button.setAttribute('disabled', 'disabled');
                });

                headers.forEach(function(header) {
                    if (awardCheckbox.checked) {
                        header.style.display = 'none';
                    } else {
                        header.style.display = 'block';
                    }
                });
        
                publications.forEach(function(publication) {
                    var hasAward = publication.querySelector('.award') !== null; // Check if publication has an award
                    if (showOnlyAwarded && !hasAward) {
                        publication.style.display = 'none';
                    } else {
                        publication.style.display = 'block';
                        count++;
                    }
                    updatePublicationCounter(count);
                });
                
                // Enable/disable all buttons based on checkbox state
                allButtons.forEach(button => {
                    if (awardCheckbox.checked) {
                        button.setAttribute('disabled', 'disabled');
                    } else {
                        button.removeAttribute('disabled');
                    }
                });

                // Reset year buttons to "All" when checkbox is checked
                if (awardCheckbox.checked) {
                    document.querySelectorAll('.filter-buttons .filter-button').forEach(button => {
                        if (button.getAttribute('data-year') === 'all') {
                            button.classList.add('active');
                        } else {
                            button.classList.remove('active');
                        }
                    });
                }

                // Reset type buttons to "All" when checkbox is checked
                if (awardCheckbox.checked) {
                    document.querySelectorAll('.filter-buttons-type .filter-button').forEach(button => {
                        if (button.getAttribute('data-type') === 'all-type') {
                            button.classList.add('active');
                        } else {
                            button.classList.remove('active');
                        }
                    });
                }
            });

            document.getElementById('q1-checkbox').addEventListener('change', function() {
                let count = 0;
                var showOnlyQ1 = document.getElementById('q1-checkbox').checked;
                var publications = document.querySelectorAll('.publication-item');
                var awardCheckbox = document.getElementById('award-checkbox');
                const q1Checkbox = this;
                
                const allButtons = document.querySelectorAll('.filter-button');

                const headers = document.querySelectorAll('.publication-type-header');

                // Disable all buttons initially
                allButtons.forEach(button => {
                    button.setAttribute('disabled', 'disabled');
                });

                headers.forEach(function(header) {
                    if (q1Checkbox.checked) {
                        header.style.display = 'none';
                    } else {
                        header.style.display = 'block';
                    }
                });
        
                publications.forEach(function(publication) {
                    var hasQ1 = publication.querySelector('.q1') !== null; // Check if publication has an award
                    console.log(hasQ1);
                    if (showOnlyQ1 && !hasQ1) {
                        publication.style.display = 'none';
                    } else {
                        publication.style.display = 'block';
                        count++;
                    }
                    updatePublicationCounter(count);
                });
                
                // Enable/disable all buttons based on checkbox state
                allButtons.forEach(button => {
                    if (q1Checkbox.checked) {
                        button.setAttribute('disabled', 'disabled');
                    } else {
                        button.removeAttribute('disabled');
                    }
                });

                // Reset year buttons to "All" when checkbox is checked
                if (q1Checkbox.checked) {
                    document.querySelectorAll('.filter-buttons .filter-button').forEach(button => {
                        if (button.getAttribute('data-year') === 'all') {
                            button.classList.add('active');
                        } else {
                            button.classList.remove('active');
                        }
                    });
                }

                // Reset type buttons to "All" when checkbox is checked
                if (q1Checkbox.checked) {
                    document.querySelectorAll('.filter-buttons-type .filter-button').forEach(button => {
                        if (button.getAttribute('data-type') === 'all-type') {
                            button.classList.add('active');
                        } else {
                            button.classList.remove('active');
                        }
                    });
                }
            });

    });

    document.addEventListener("DOMContentLoaded", function () {
        const publicationSearch = document.getElementById("publication-search");
        const awardCheckbox = document.getElementById("award-checkbox");
        const q1Checkbox = document.getElementById("q1-checkbox");
        let showHeader = false; // Variable to track if the header should be shown
        
        publicationSearch.addEventListener("input", function () {
            const searchQuery = this.value.trim().toLowerCase();
            awardCheckbox.checked = false; 
            q1Checkbox.checked = false; 
            filterPublicationsSearch(searchQuery);
            updatePublicationCounter();
        });

        function filterPublicationsSearch(searchQuery) {
            const publicationItems = document.querySelectorAll(".publication-item");

            const allButtons = document.querySelectorAll('.filter-button');
            const yearDropdown = document.getElementById("year-dropdown");
            const typeDropdown = document.getElementById("type-dropdown");

            const award = document.querySelector(".award-checkbox");
            const q1 = document.querySelector(".q1-checkbox");

            // Disable all buttons initially
            allButtons.forEach(button => {
                button.classList.add('active');
            });

            yearDropdown.selectedIndex = 0; // Set the first option as selected
            typeDropdown.selectedIndex = 0;

            // Reset year buttons to "All" when checkbox is checked
            document.querySelectorAll('.filter-buttons .filter-button').forEach(button => {
                if (button.getAttribute('data-year') === 'all') {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });

            // Reset type buttons to "All" when checkbox is checked
            document.querySelectorAll('.filter-buttons-type .filter-button').forEach(button => {
                if (button.getAttribute('data-type') === 'all-type') {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });

            publicationItems.forEach(item => {
                const title = item.querySelector(".publication-title").textContent.toLowerCase();
                const authors = Array.from(item.querySelectorAll(".author")).map(author => author.textContent.toLowerCase()).join(", ");

                if (title.includes(searchQuery) || authors.includes(searchQuery)) {
                    item.style.display = "block"; // Show publication item
                } else {
                    item.style.display = "none"; // Hide publication item
                }
            });
        }
    });
</script>
                    </div>
                    
                </div>
            </div>
        </section>
        
            <footer class="footer">
    <div class="container">
        
        

        <div class="content is-small has-text-centered">
            <p class="">Email contact: gianluca.moro[at]unibo.it</p>
        </div>
    </div>
</footer>

        
        <script src="/assets/js/app.js" type="text/javascript"></script><!-- footer scripts -->
</body>
</html>
