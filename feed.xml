<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://nlp.apice.unibo.it/feed.xml" rel="self" type="application/atom+xml" /><link href="http://nlp.apice.unibo.it/" rel="alternate" type="text/html" /><updated>2025-02-27T21:54:28+00:00</updated><id>http://nlp.apice.unibo.it/feed.xml</id><title type="html">UniboNLP</title><subtitle>The Unibo Natural Language Processing Group (UniboNLP), led by prof. Gianluca Moro, includes a large team of Ph.D. students and researchers which are part of the Department of Computer Science and Engineering (DISI) of the University of Bologna.</subtitle><entry><title type="html">EMNLP 2024</title><link href="http://nlp.apice.unibo.it/development/2024/10/17/emnlp24/" rel="alternate" type="text/html" title="EMNLP 2024" /><published>2024-10-17T22:25:00+00:00</published><updated>2024-10-17T22:25:00+00:00</updated><id>http://nlp.apice.unibo.it/development/2024/10/17/emnlp24</id><content type="html" xml:base="http://nlp.apice.unibo.it/development/2024/10/17/emnlp24/">&lt;p&gt;We are proud to announce that our group will be at EMNLP 2024 with an accepted long paper in the Main Track!&lt;/p&gt;

&lt;h4&gt;Unknown Claims: Generation of Fact-Checking Training Examples from Unstructured and Structured Data&lt;/h4&gt;
&lt;h5&gt;by JF Bussotti, L. Ragazzi, G. Frisoni, G. Moro and P. Papotti&lt;/h5&gt;

&lt;p&gt;We will attend in presence in Miami and present &lt;b&gt;Unown&lt;/b&gt;, a novel domain-agnostic data generation framework for fact-checking systems that integrate both textual and tabular content.&lt;/p&gt;
&lt;p&gt;
Computational fact-checking (FC) relies on supervised models to verify claims based on given evidence, requiring a resource-intensive process to annotate large volumes of training data. We introduce Unown, a novel framework that generates training instances for FC systems automatically using both textual and tabular content. Unown selects relevant evidence and generates supporting and refuting claims with advanced negation artifacts. Designed to be flexible, Unown accommodates various strategies for evidence selection and claim generation, offering unparalleled adaptability. We comprehensively evaluate Unown on both text-only and table+text benchmarks, including Feverous, SciFact, and MMFC, a new multi-modal FC dataset. Our results prove that Unown examples are of comparable quality to expert-labeled data, even enabling models to achieve up to 5% higher accuracy. The code, data, and models are available at https://github.com/disi-unibo-nlp/unown.
&lt;/p&gt;</content><author><name></name></author><summary type="html">We are proud to announce that our group will be at EMNLP 2024 with an accepted long paper in the Main Track!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://github.com/acl-org/emnlp-2024/blob/main/assets/images/miami/EMNLP_2024_Website_Image.png?raw=true" /><media:content medium="image" url="https://github.com/acl-org/emnlp-2024/blob/main/assets/images/miami/EMNLP_2024_Website_Image.png?raw=true" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">ACL 2024</title><link href="http://nlp.apice.unibo.it/development/2024/05/17/acl24/" rel="alternate" type="text/html" title="ACL 2024" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>http://nlp.apice.unibo.it/development/2024/05/17/acl24</id><content type="html" xml:base="http://nlp.apice.unibo.it/development/2024/05/17/acl24/">&lt;p&gt;We are proud to announce that our group will be at ACL 2024 with two accepted papers in the Main Track and Findings!&lt;/p&gt;

&lt;p&gt;Here’s the list of the papers:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering&lt;/li&gt;
  &lt;li&gt;What Are You Token About? Differentiable Perturbed Top-k Token Selection for Scientific Document Summarization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h4&gt;To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering&lt;/h4&gt;
&lt;h5&gt;by G. Frisoni, A. Cocchieri, A. Presepi, G. Moro and Z. Meng&lt;/h5&gt;
&lt;p&gt;
Medical open-domain question answering demands substantial access to specialized knowledge. Recent efforts have sought to decouple knowledge from model parameters, counteracting architectural scaling and allowing for training on common low-resource hardware. The retrieve-then-read paradigm has become ubiquitous, with model predictions grounded on relevant knowledge pieces from external repositories such as PubMed, textbooks, and UMLS. An alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting. As a result, 'to generate or to retrieve' is the modern equivalent of Hamlet’s dilemma. This paper presents MEDGENIE, the first generate-then-read framework for multiple-choice question answering in medicine. We conduct extensive experiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical perspective by assuming a maximum of 24GB VRAM. MEDGENIE sets a new state-of-the- art (SOTA) in the open-book setting of each testbed, even allowing a small-scale reader to outcompete zero-shot closed-book 175B baselines while using up to 706× fewer parameters. Overall, our findings reveal that generated passages are more effective than retrieved counterparts in attaining higher accuracy.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;a class=&quot;link&quot; href=&quot;https://arxiv.org/pdf/2403.01924&quot;&gt;Check out the paper!&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h4&gt;What Are You Token About? Differentiable Perturbed Top-k Token Selection for Scientific Document Summarization&lt;/h4&gt;
&lt;h5&gt;by L. Ragazzi, P. Italiani, G. Moro and M. Panni&lt;/h5&gt;
&lt;p&gt;
Scientific document summarization (SDS) aims to condense complex and long articles in both technical and plain-language terms to facilitate the accessibility and dissemination of scientific findings. Existing datasets lack source heterogeneity, hindering effective model training and generalizability. First, we introduce SciLay, a novel dataset that includes documents from multiple natural science journals with expert-authored technical and lay summaries. Second, we propose PrunePert, a new transformer-based model that incorporates a differentiable perturbed top-k encoder layer to prune irrelevant tokens in end-to-end learning. Experimental results show that our model achieves a nearly 2x speed-up compared to a state-of-the-art linear transformer, remaining comparable in effectiveness. Additional examinations underscore the importance of employing a training dataset that includes different sources to enhance the generalizability of the models. Code is available at https://anonymous.4open.science/r/sci-lay-2511/.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;a class=&quot;link&quot; href=&quot;https://aclanthology.org/2024.findings-acl.561/&quot;&gt;Check out the paper!&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">We are proud to announce that our group will be at ACL 2024 with two accepted papers in the Main Track and Findings!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://github.com/acl-org/acl-2024/blob/main/assets/images/bangkok/bangkok-banner.jpeg?raw=true" /><media:content medium="image" url="https://github.com/acl-org/acl-2024/blob/main/assets/images/bangkok/bangkok-banner.jpeg?raw=true" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">LLM Slide Deck</title><link href="http://nlp.apice.unibo.it/development/2024/04/17/llm-slides/" rel="alternate" type="text/html" title="LLM Slide Deck" /><published>2024-04-17T00:00:00+00:00</published><updated>2024-04-17T00:00:00+00:00</updated><id>http://nlp.apice.unibo.it/development/2024/04/17/llm-slides</id><content type="html" xml:base="http://nlp.apice.unibo.it/development/2024/04/17/llm-slides/">&lt;p&gt;🤖 Dive into the cutting-edge world of Large Language Models (LLMs) with our latest slide deck!&lt;/p&gt;

&lt;p&gt;Developed in Text Mining and NLP, LLMs are driving a breakthrough in AI and human-machine interaction. They impact a multitude of tasks and domains and are poised to revolutionize society as a whole. In a future where tokens function as currency, mastering and implementing these subjects is an imperative skill in academic and industrial contexts.&lt;/p&gt;

&lt;p&gt;Our research group has curated a short review of the state of the art within the LLM literature.&lt;/p&gt;

&lt;p&gt;🔎 Explore topics such as:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;LLM applications&lt;/li&gt;
  &lt;li&gt;Preliminaries (e.g., transformers, scaling laws, emerging abilities)&lt;/li&gt;
  &lt;li&gt;Open issues (e.g., black-box behavior, training costs, hallucinations, knowledge recency and editing, lexical superficiality, human reporting bias, reasoning inconsistency)&lt;/li&gt;
  &lt;li&gt;Developer perspective
    &lt;ul&gt;
      &lt;li&gt;Models&lt;/li&gt;
      &lt;li&gt;Datasets&lt;/li&gt;
      &lt;li&gt;Prompting&lt;/li&gt;
      &lt;li&gt;Training techniques (e.g., instruction fine-tuning, adapter fine-tuning, mixture-of-experts)&lt;/li&gt;
      &lt;li&gt;Optimization&lt;/li&gt;
      &lt;li&gt;Retrieval-augmented generation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, we briefly introduce the contributions of our group in the last three years.&lt;/p&gt;

&lt;p&gt;Ready to level up your understanding of LLMs?&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;a class=&quot;link&quot; href=&quot;https://www.dropbox.com/scl/fi/hdn98k7paq0yca91zovow/LLM_NLP_Group_DISI_UniBO_Cesena_Campus_v2.pdf?rlkey=d2yw9x8l25glo3317i7cznov4&amp;amp;dl=0&quot;&gt;Link to the slides&lt;/a&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;a class=&quot;link&quot; href=&quot;https://www.dropbox.com/scl/fi/rtry6md8xzasr9lsyxv88/LLM_NLP_Group_DISI_UniBO_Cesena_Campus_2x.pdf?rlkey=5xb27etx4o4xfle94ubn0cu2p&amp;amp;dl=0&quot;&gt;Compact printable version&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;#NLP #AI #GenerativeAI #LLMs #Research #Innovation&lt;/p&gt;</content><author><name></name></author><summary type="html">🤖 Dive into the cutting-edge world of Large Language Models (LLMs) with our latest slide deck!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://drive.google.com/thumbnail?sz=w1920&amp;id=1eVN7Cw3mBZfrQXwYGT0jNjoAMMfPsnt5" /><media:content medium="image" url="https://drive.google.com/thumbnail?sz=w1920&amp;id=1eVN7Cw3mBZfrQXwYGT0jNjoAMMfPsnt5" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">ICLR 2024</title><link href="http://nlp.apice.unibo.it/development/2024/02/16/iclr24/" rel="alternate" type="text/html" title="ICLR 2024" /><published>2024-02-16T00:00:00+00:00</published><updated>2024-02-16T00:00:00+00:00</updated><id>http://nlp.apice.unibo.it/development/2024/02/16/iclr24</id><content type="html" xml:base="http://nlp.apice.unibo.it/development/2024/02/16/iclr24/">&lt;p&gt;We are proud to announce that our group has one paper accepted at ICLR 2024 in the Tiny Track!&lt;/p&gt;

&lt;h4&gt;Revelio: Interpretable Long-Form Question Answering&lt;/h4&gt;
&lt;h5&gt;by G. Moro, L. Ragazzi, L. Valgimigli, F. Vincenzi, and D. Freddi&lt;/h5&gt;
&lt;p&gt;
The black-box architecture of pretrained language models (PLMs) hinders the interpretability of lengthy responses in long-form question answering (LFQA). Prior studies use knowledge graphs (KGs) to enhance output transparency, but mostly focus on non-generative or short-form QA. We present Revelio, a new layer that maps PLM's inner working onto a KG walk. Tests on two LFQA datasets show that Revelio supports PLM-generated answers with reasoning paths presented as rationales while retaining performance and time akin to their vanilla counterparts.


&lt;/p&gt;</content><author><name></name></author><summary type="html">We are proud to announce that our group has one paper accepted at ICLR 2024 in the Tiny Track!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://media.iclr.cc/Conferences/ICLR2024/img/prater.jpg" /><media:content medium="image" url="https://media.iclr.cc/Conferences/ICLR2024/img/prater.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Neurocomputing</title><link href="http://nlp.apice.unibo.it/development/2023/09/30/neurocomputing/" rel="alternate" type="text/html" title="Neurocomputing" /><published>2023-09-30T00:00:00+00:00</published><updated>2023-09-30T00:00:00+00:00</updated><id>http://nlp.apice.unibo.it/development/2023/09/30/neurocomputing</id><content type="html" xml:base="http://nlp.apice.unibo.it/development/2023/09/30/neurocomputing/">&lt;p&gt;We are proud to announce that our group has two long papers accepted at the Neurocomputing journal!&lt;/p&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;“Align-Then-Abstract Representation Learning for Low-Resource Summarization” (&lt;a class=&quot;link&quot; href=&quot;https://www.sciencedirect.com/science/article/pii/S0925231223004794?via%3Dihub&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;“Efficient Text-Image Semantic Search: a Multi-modal Vision-Language Approach for Fashion Retrieval” (&lt;a class=&quot;link&quot; href=&quot;https://www.sciencedirect.com/science/article/pii/S092523122300303X?via%3Dihub&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">We are proud to announce that our group has two long papers accepted at the Neurocomputing journal!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ars.els-cdn.com/content/image/X09252312.jpg" /><media:content medium="image" url="https://ars.els-cdn.com/content/image/X09252312.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">ECAI 2023</title><link href="http://nlp.apice.unibo.it/development/2023/09/30/ecai23/" rel="alternate" type="text/html" title="ECAI 2023" /><published>2023-09-30T00:00:00+00:00</published><updated>2023-09-30T00:00:00+00:00</updated><id>http://nlp.apice.unibo.it/development/2023/09/30/ecai23</id><content type="html" xml:base="http://nlp.apice.unibo.it/development/2023/09/30/ecai23/">&lt;p&gt;We are proud to announce that our group will be at ECAI 2023 with an accepted long paper in the Main Track!&lt;/p&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;“Graph-Based Abstractive Summarization of Extracted Essential Knowledge for Low-Resource Scenarios” (Main Track)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are interested check https://github.com/disi-unibo-nlp/gseek.&lt;/p&gt;</content><author><name></name></author><summary type="html">We are proud to announce that our group will be at ECAI 2023 with an accepted long paper in the Main Track!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://groups.google.com/group/ml-news/attach/1dc190f261d93/image001.png?part=0.1&amp;view=1" /><media:content medium="image" url="https://groups.google.com/group/ml-news/attach/1dc190f261d93/image001.png?part=0.1&amp;view=1" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Artificial Intelligence and Law</title><link href="http://nlp.apice.unibo.it/development/2023/09/29/ailaw/" rel="alternate" type="text/html" title="Artificial Intelligence and Law" /><published>2023-09-29T00:00:00+00:00</published><updated>2023-09-29T00:00:00+00:00</updated><id>http://nlp.apice.unibo.it/development/2023/09/29/ailaw</id><content type="html" xml:base="http://nlp.apice.unibo.it/development/2023/09/29/ailaw/">&lt;p&gt;We are proud to announce that our group has one long paper accepted at AI&amp;amp;Law!&lt;/p&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;“Multi-language transfer learning for low-resource legal case summarization”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a class=&quot;link&quot; href=&quot;https://link.springer.com/article/10.1007/s10506-023-09373-8&quot;&gt;Check out the paper!&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">We are proud to announce that our group has one long paper accepted at AI&amp;amp;Law!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://media.springernature.com/full/springer-static/cover-hires/journal/10506" /><media:content medium="image" url="https://media.springernature.com/full/springer-static/cover-hires/journal/10506" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">AAAI 2023</title><link href="http://nlp.apice.unibo.it/development/2023/02/07/aaai23/" rel="alternate" type="text/html" title="AAAI 2023" /><published>2023-02-07T10:50:07+00:00</published><updated>2023-02-07T10:50:07+00:00</updated><id>http://nlp.apice.unibo.it/development/2023/02/07/aaai23</id><content type="html" xml:base="http://nlp.apice.unibo.it/development/2023/02/07/aaai23/">&lt;p&gt;We are proud to announce that our group will be at AAAI 2023 in Washington with two accepted long paper in the Main Track and AI for Social Impact!&lt;/p&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;“Cogito Ergo Summ: Abstractive Summarization of Biomedical Papers via Semantic Parsing Graphs and Consistency Rewards” (Main Track)&lt;/li&gt;
  &lt;li&gt;“Carburacy: Summarization Models Tuning and Comparison in Eco-Sustainable Regimes with a Novel Carbon-Aware Accuracy” (Artificial Intelligence for Social Impact track)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are interested check https://github.com/disi-unibo-nlp/cogito-ergo-summ and https://github.com/disi-unibo-nlp/carburacy.&lt;/p&gt;</content><author><name></name></author><summary type="html">We are proud to announce that our group will be at AAAI 2023 in Washington with two accepted long paper in the Main Track and AI for Social Impact!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://aaai-23.aaai.org/wp-content/uploads/2022/12/AAAI-23-Website-Banner_2560x593-min.png" /><media:content medium="image" url="https://aaai-23.aaai.org/wp-content/uploads/2022/12/AAAI-23-Website-Banner_2560x593-min.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">EMNLP 2022</title><link href="http://nlp.apice.unibo.it/development/2022/12/07/emnlp22/" rel="alternate" type="text/html" title="EMNLP 2022" /><published>2022-12-07T23:25:00+00:00</published><updated>2022-12-07T23:25:00+00:00</updated><id>http://nlp.apice.unibo.it/development/2022/12/07/emnlp22</id><content type="html" xml:base="http://nlp.apice.unibo.it/development/2022/12/07/emnlp22/">&lt;p&gt;We are proud to announce that our group will be at EMNLP 2022 with an accepted long paper in the Main Track!&lt;/p&gt;

&lt;h4&gt;BioReader: a Retrieval-Enhanced Text-to-Text Transformer for Biomedical Literature&lt;/h4&gt;
&lt;h5&gt;by G. Frisoni, M. Mizutani, G. Moro and L. Valgimigli&lt;/h5&gt;

&lt;p&gt;We will attend in presence and present &lt;b&gt;BioReader&lt;/b&gt;, the first retrieval-enhanced transformer for biomedical literature.&lt;/p&gt;
&lt;p&gt;
The latest batch of research has equipped language models with the ability to attend over relevant and factual information from non-parametric external sources, drawing a complementary path to architectural scaling. Besides mastering language, exploiting and contextualizing the latent world knowledge is crucial in complex domains like biomedicine. However, most works in the field rely on general-purpose models supported by databases like Wikipedia and Books. We introduce BioReader, the first retrieval-enhanced text-to-text model for biomedical natural language processing. Our domain-specific T5-based solution augments the input prompt by fetching and assembling relevant scientific literature chunks from a neural database with ≈60 million tokens centered on PubMed. We fine-tune and evaluate BioReader on a broad array of downstream tasks, significantly outperforming several state-of-the-art methods despite using up to 3x fewer parameters. In tandem with extensive ablation studies, we show that domain knowledge can be easily altered or supplemented to make the model generate correct predictions bypassing the retraining step and thus addressing the literature overload issue.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;a class=&quot;link&quot; href=&quot;https://aclanthology.org/2022.emnlp-main.390/&quot;&gt;Check out the paper!&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">We are proud to announce that our group will be at EMNLP 2022 with an accepted long paper in the Main Track!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.vinai.io/wp-content/uploads/2022/10/EMNLP.png" /><media:content medium="image" url="https://www.vinai.io/wp-content/uploads/2022/10/EMNLP.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">COLING 2022</title><link href="http://nlp.apice.unibo.it/development/2022/09/30/coling22/" rel="alternate" type="text/html" title="COLING 2022" /><published>2022-09-30T23:25:00+00:00</published><updated>2022-09-30T23:25:00+00:00</updated><id>http://nlp.apice.unibo.it/development/2022/09/30/coling22</id><content type="html" xml:base="http://nlp.apice.unibo.it/development/2022/09/30/coling22/">&lt;p&gt;We are proud to announce that our group will be at COLING 2022 with two accepted long papers in the Main Track!&lt;/p&gt;

&lt;p&gt;Here’s the list of the papers:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Text-to-Text Extraction and Verbalization of Biomedical Event Graphs&lt;/li&gt;
  &lt;li&gt;NLG-Metricverse: An End-to-End Library for Evaluating Natural Language Generation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h4&gt;Text-to-Text Extraction and Verbalization of Biomedical Event Graphs&lt;/h4&gt;
&lt;h5&gt;by G. Frisoni, G. Moro and L. Balzani&lt;/h5&gt;
&lt;p&gt;We will present our works on &lt;b&gt;Event Extraction&lt;/b&gt;, &lt;b&gt;Graph Verbalization&lt;/b&gt;, and &lt;b&gt;Natural Language Generation Evaluation&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;
Biomedical events represent complex, graphical, and semantically rich interactions expressed in the scientific literature. Almost all contributions in the event realm orbit around semantic parsing, usually employing discriminative architectures and cumbersome multi-step pipelines limited to a small number of target interaction types. We present the first lightweight framework to solve both event extraction and event verbalization with a unified text-to-text approach, allowing us to fuse all the resources so far designed for different tasks. To this end, we present a new event graph linearization technique and release highly comprehensive event-text paired datasets, covering more than 150 event types from multiple biology subareas (English language). By streamlining parsing and generation to translations, we propose baseline transformer model results according to multiple biomedical text mining benchmarks and natural language generation metrics. Our extractive models achieve greater state-of-the-art performance than single-task competitors and show promising capabilities for the controlled generation of coherent natural language utterances from structured data.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;a class=&quot;link&quot; href=&quot;https://aclanthology.org/2022.coling-1.238/&quot;&gt;Check out the paper!&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h4&gt;NLG-Metricverse: An End-to-End Library for Evaluating Natural Language Generation&lt;/h4&gt;
&lt;h5&gt;by G. Frisoni, A. Carbonaro, G. Moro, A. Zammarchi and M. Avagnano&lt;/h5&gt;
&lt;p&gt;
Driven by deep learning breakthroughs, natural language generation (NLG) models have been at the center of steady progress in the last few years, with a ubiquitous task influence. However, since our ability to generate human-indistinguishable artificial text lags behind our capacity to assess it, it is paramount to develop and apply even better automatic evaluation metrics. To facilitate researchers to judge the effectiveness of their models broadly, we introduce NLG-Metricverse—an end-to-end open-source library for NLG evaluation based on Python. Our framework provides a living collection of NLG metrics in a unified and easy-to-use environment, supplying tools to efficiently apply, analyze, compare, and visualize them. This includes (i) the extensive support to heterogeneous automatic metrics with n-arity management, (ii) the meta-evaluation upon individual performance, metric-metric and metric-human correlations, (iii) graphical interpretations for helping humans better gain score intuitions, (iv) formal categorization and convenient documentation to accelerate metrics understanding. NLG-Metricverse aims to increase the comparability and replicability of NLG research, hopefully stimulating new contributions in the area.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;a class=&quot;link&quot; href=&quot;https://aclanthology.org/2022.coling-1.306/&quot;&gt;Check out the paper!&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">We are proud to announce that our group will be at COLING 2022 with two accepted long papers in the Main Track!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://static1.s123-cdn-static-a.com/uploads/4165719/800_62fc19c314a5c.jpg" /><media:content medium="image" url="https://static1.s123-cdn-static-a.com/uploads/4165719/800_62fc19c314a5c.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>