<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="https://disi-unibo-nlp.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://disi-unibo-nlp.github.io/" rel="alternate" type="text/html" /><updated>2025-07-18T13:25:43+00:00</updated><id>https://disi-unibo-nlp.github.io/feed.xml</id><title type="html">UniboNLP</title><subtitle>The Unibo Natural Language Processing Group</subtitle><entry><title type="html">ECAI 2025</title><link href="https://disi-unibo-nlp.github.io/development/2025/07/16/ecai25/" rel="alternate" type="text/html" title="ECAI 2025" /><published>2025-07-16T09:55:00+00:00</published><updated>2025-07-16T09:55:00+00:00</updated><id>https://disi-unibo-nlp.github.io/development/2025/07/16/ecai25</id><content type="html" xml:base="https://disi-unibo-nlp.github.io/development/2025/07/16/ecai25/">&lt;p&gt;We are proud to announce that our group will be at ECAI 2025 with &lt;b&gt;3 long papers: 2 works in the Main track and 1 in the Demonstrations track!&lt;/b&gt; Catch us in &lt;b&gt;Bologna, Italy&lt;/b&gt; to learn more about &lt;b&gt;automatic prompt learning&lt;/b&gt;, &lt;b&gt;hierarchical extreme multi-label text classification in the food domain&lt;/b&gt;, and &lt;b&gt;protein language models&lt;/b&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4&gt;&quot;Magic Mirror on the Wall, Which is the Fairest Prompt of All? A Survey on Automatic Prompt Learning&lt;/h4&gt;
&lt;p style=&quot;color:gray; margin-top:-10px;&quot;&gt;by S. Fantazzini, G. Frisoni, G. Moro, L. Ragazzi, M. Ciccioni, and C. Sartori&lt;/p&gt;

&lt;p&gt;
Prompts direct the behavior of a model by conditioning its outputs on carefully designed instructions and examples, similar to setting the trajectory of an arrow before release. More broadly, prompt learning is the research area that aims to solve downstream tasks by directly leveraging the knowledge acquired by language models at pre-training time, removing the need for expensive fine-tuning stages with potentially different objective functions. While manual prompt engineering has enabled both small and large language models to achieve superhuman performance on numerous benchmarks, it remains a labor-intensive and suboptimal process. Recently, the field has shifted towards automating the search for prompts that effectively elicit the desired model responses. This survey presents the first systematic review of prompt learning for pre-trained language models operating on text inputs, with a particular focus on automatic methods. We critically analyze existing publications and organize them into a novel taxonomy, describing key aspects for practical usage. We finally discuss promising directions for future research. Our curated repository of annotated papers, continuously updated, is available at https://anonymous.4open.science/r/awesome-prompt-learning.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;i&gt;The paper will be available soon!&lt;/i&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4&gt;FEAST: Retrieval-Augmented Multi-Hierarchical Food Classification for the FoodEx2 System&lt;/h4&gt;
&lt;p style=&quot;color:gray; margin-top:-10px;&quot;&gt;by L. Molfetta, A. Cocchieri, S. Fantazzini, G. Frisoni, L. Ragazzi, and G. Moro&lt;/p&gt;

&lt;p&gt;
Hierarchical text classification (HTC) and extreme multi-label classification (XML) tasks face compounded challenges from complex label interdependencies, data sparsity, and extreme output dimensions. These challenges are exemplified in the European Food Safety Authority's FoodEx2 system—a standardized food classification framework essential for food consumption monitoring and contaminant exposure assessment across Europe. FoodEx2 coding transforms natural language food descriptions into a set of codes from multiple standardized hierarchies, but faces implementation barriers due to its complex structure. Given a food description (e.g., &quot;organic yogurt&quot;), the system identifies its base term (&quot;yogurt&quot;), all the applicable facet categories (e.g., &quot;production method&quot;), and then, every relevant facet descriptors to each category (e.g., &quot;organic production&quot;). While existing approaches perform adequately on well-balanced and semantically dense hierarchies, no work has been applied on the practical constraints imposed by the FoodEx2 system. The limited literature addressing such real-world scenarios further compounds these challenges. We propose FEAST (Food Embedding And Semantic Taxonomy), a novel retrieval-augmented framework that decomposes the FoodEx2 classification challenge into a three-stage approach: (1) base term identification, (2) multi-label facet prediction, and (3) facet descriptor assignment. By leveraging the system's hierarchical structure to guide training and performing deep metric learning, FEAST learns discriminative embeddings that mitigate data sparsity and improve generalization on rare and fine-grained labels. Evaluated on the multilingual FoodEx2 benchmark, FEAST outperforms the prior European's CNN baseline F1 scores by 12—38\% on rare classes. Code and models are released to support reproducibility at https://anonymous.4open.science/r/foodex2-coding-6741/
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;i&gt;The paper will be available soon!&lt;/i&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4&gt;Predicting Protein Functions with Ensemble Deep Learning and Protein Language Models&lt;/h4&gt;
&lt;p style=&quot;color:gray; margin-top:-10px;&quot;&gt;by G. Frisoni, M. Fuschi, and G. Moro&lt;/p&gt;

&lt;p&gt;
Understanding protein functions enables deciphering cellular mechanisms and improving healthcare outcomes, from disease diagnosis to targeted therapy. We present GOMix, an ensemble learning method for predicting the functions of newly discovered proteins, packaged within an easy-to-use web application. By combining seven complementary base predictors---including sequence homology and protein language models, GOMix achieves competitive or state-of-the-art performance in the CAFA-3 challenge. Unlike existing solutions, GOMix is entirely open-source, modular, and computationally low-resource. The code is publicly available at https://github.com/disi-unibo-nlp/gomix (MIT License).
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;i&gt;The paper will be available soon!&lt;/i&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">We are proud to announce that our group will be at ECAI 2025 with 3 long papers: 2 works in the Main track and 1 in the Demonstrations track! Catch us in Bologna, Italy to learn more about automatic prompt learning, hierarchical extreme multi-label text classification in the food domain, and protein language models.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://aiconferences.ai/wp-content/uploads/2024/12/ECAI-2025.png" /><media:content medium="image" url="https://aiconferences.ai/wp-content/uploads/2024/12/ECAI-2025.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">ACL 2025</title><link href="https://disi-unibo-nlp.github.io/development/2025/05/20/acl25/" rel="alternate" type="text/html" title="ACL 2025" /><published>2025-05-20T09:55:00+00:00</published><updated>2025-05-20T09:55:00+00:00</updated><id>https://disi-unibo-nlp.github.io/development/2025/05/20/acl25</id><content type="html" xml:base="https://disi-unibo-nlp.github.io/development/2025/05/20/acl25/">&lt;p&gt;We are delighted to announce that we will be at ACL 2025 with &lt;b&gt;2 long papers in the Main and Findings Track!&lt;/b&gt; Catch us in &lt;b&gt;Vienna, Austria&lt;/b&gt; to learn more about &lt;b&gt;humor QA&lt;/b&gt; and &lt;b&gt;biomedical NER&lt;/b&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4&gt;&quot;What do you call a dog that is incontrovertibly true? Dogma&quot;: Testing LLM Generalization through Humor&lt;/h4&gt;
&lt;p style=&quot;color:gray; margin-top:-10px;&quot;&gt;by A. Cocchieri, L. Ragazzi, P. Italiani, G. Tagliavini, and G. Moro&lt;/p&gt;

&lt;p&gt;
Humor, requiring creativity and contextual understanding, is a hallmark of human intelligence, showcasing adaptability across linguistic scenarios. While recent advances in large language models (LLMs) demonstrate strong reasoning on various benchmarks, it remains unclear whether they truly adapt to new tasks like humans (i.e., generalize) or merely replicate memorized content. To explore this, we introduce Phunny, a new humor-based question-answering benchmark designed to assess LLMs' reasoning through carefully crafted puns. Our dataset is manually curated to ensure novelty and minimize data contamination, providing a robust evaluation of LLMs' linguistic comprehension. Experiments on pun comprehension, resolution, and generation reveal that most LLMs struggle with generalization, even on simple tasks, consistently underperforming the human baseline. Additionally, our detailed error analysis provides valuable insights to guide future research. The data is available at https://anonymous.4open.science/r/phunny/.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;i&gt;The paper will be available soon!&lt;/i&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4&gt;ZeroNER: Fueling Zero-Shot Named Entity Recognition via Entity Type Descriptions&lt;/h4&gt;
&lt;p style=&quot;color:gray; margin-top:-10px;&quot;&gt;by A. Cocchieri, M. M. Galindo, G. Frisoni, G. Moro, C. Sartori, and G. Tagliavini&lt;/p&gt;

&lt;p&gt;
In real-world Named Entity Recognition (NER), annotation scarcity and the challenge of unseen entity types make zero-shot learning essential. While Large Language Models (LLMs) possess vast parametric knowledge, they fall short in cost-effectiveness compared to specialized encoders. Current zero-shot methods often rely solely on entity type names, overlooking both the critical role of descriptions in resolving definition ambiguities and the issue of type leakage during pretraining. In this work, we introduce ZeroNER, a description-driven framework that enhances zero-shot NER in low-resource settings. By leveraging entity type descriptions through cross-attention, ZeroNER enables a BERT-based student model to identify any entity type without additional training. Evaluated on three real-world zero-shot benchmarks under a rigorous hard zero-shot setting, ZeroNER consistently outperforms several LLMs by up to 15% in F1 score and surpasses alternative lightweight methods that rely solely on type names. Furthermore, our findings reveal that many LLMs significantly benefit from using type descriptions, underscoring their potential in zero-shot NER.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;i&gt;The paper will be available soon!&lt;/i&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">We are delighted to announce that we will be at ACL 2025 with 2 long papers in the Main and Findings Track! Catch us in Vienna, Austria to learn more about humor QA and biomedical NER.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://nlp.jhu.edu/multivent/resources/acl.png" /><media:content medium="image" url="https://nlp.jhu.edu/multivent/resources/acl.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">IJCAI 2025</title><link href="https://disi-unibo-nlp.github.io/development/2025/04/29/ijcai25/" rel="alternate" type="text/html" title="IJCAI 2025" /><published>2025-04-29T13:55:00+00:00</published><updated>2025-04-29T13:55:00+00:00</updated><id>https://disi-unibo-nlp.github.io/development/2025/04/29/ijcai25</id><content type="html" xml:base="https://disi-unibo-nlp.github.io/development/2025/04/29/ijcai25/">&lt;p&gt;We are glad to share that our group will be at IJCAI 2025 with &lt;b&gt;1 long paper in the Survey Track!&lt;/b&gt; Catch us in &lt;b&gt;Montreal, Canada&lt;/b&gt; to learn more about &lt;b&gt;neuro-symbolic artificial intelligence&lt;/b&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4&gt;Neuro-Symbolic Artificial Intelligence: A Task-Directed Survey in the Black-Box Era&lt;/h4&gt;
&lt;p style=&quot;color:gray; margin-top:-10px;&quot;&gt;by G. P. Delvecchio, L. Molfetta, and G. Moro&lt;/p&gt;
&lt;p&gt;
The integration of symbolic computing with neural networks has intrigued researchers since the first theorizations of Artificial intelligence (AI). The ability of Neuro-Symbolic (NeSy) methods to infer or exploit behavioral schema has been widely considered as one of the possible proxies for human-level intelligence. However, the limited semantic generalizability and the challenges in declining complex domains with pre-defined patterns and rules hinder their practical implementation in real-world scenarios. The unprecedented results achieved by connectionist systems since the last AI breakthrough in 2017 have raised questions about the competitiveness of NeSy solutions, with particular emphasis on the Natural Language Processing and Computer Vision fields. This survey examines task-specific advancements in the NeSy domain to explore how incorporating symbolic systems can enhance explainability and reasoning capabilities. Our findings are meant to serve as a resource for researchers exploring explainable NeSy methodologies for real-life tasks and applications. Reproducibility details and in-depth comments on each surveyed research work are made available at https://github.com/disi-unibo-nlp/task-oriented-neuro-symbolic.git.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;i&gt;The paper will be available soon!&lt;/i&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">We are glad to share that our group will be at IJCAI 2025 with 1 long paper in the Survey Track! Catch us in Montreal, Canada to learn more about neuro-symbolic artificial intelligence.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cdn.bsky.app/img/banner/plain/did:plc:ooxfdkl252t46ibrpn7wn36v/bafkreihooxoc3i5l3jaz4xkumbsfjnrufyo52sfr4kwxulvpjctezzcye4@jpeg" /><media:content medium="image" url="https://cdn.bsky.app/img/banner/plain/did:plc:ooxfdkl252t46ibrpn7wn36v/bafkreihooxoc3i5l3jaz4xkumbsfjnrufyo52sfr4kwxulvpjctezzcye4@jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">NAACL 2025</title><link href="https://disi-unibo-nlp.github.io/development/2025/01/23/naacl25/" rel="alternate" type="text/html" title="NAACL 2025" /><published>2025-01-23T13:55:00+00:00</published><updated>2025-01-23T13:55:00+00:00</updated><id>https://disi-unibo-nlp.github.io/development/2025/01/23/naacl25</id><content type="html" xml:base="https://disi-unibo-nlp.github.io/development/2025/01/23/naacl25/">&lt;p&gt;We are proud to announce that our group will be at NAACL 2025 with &lt;b&gt;1 long paper in the Findings Track!&lt;/b&gt; Catch us in &lt;b&gt;Albuquerque, New Mexico&lt;/b&gt; to learn more about &lt;b&gt;open-domain biomedical NER&lt;/b&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4&gt;OpenBioNER: Lightweight Open-Domain Biomedical Named Entity Recognition Through Entity Type Description&lt;/h4&gt;
&lt;p style=&quot;color:gray; margin-top:-10px;&quot;&gt;by A. Cocchieri, G. Frisoni, M. Martinez Galindo, G. Moro, G. Tagliavini, and F. Candoli&lt;/p&gt;
&lt;p&gt;
Biomedical Named Entity Recognition (BioNER) faces significant challenges in real-world applications due to limited annotated data and the constant emergence of new entity types, making zero-shot learning capabilities crucial. While Large Language Models (LLMs) possess extensive domain knowledge necessary for specialized fields like biomedicine, their computational costs often make them impractical. To address these challenges, we introduce OpenBioNER, a lightweight BERT-based cross-encoder architecture that can identify any biomedical entity using only its description, eliminating the need for retraining on new, unseen entity types. Through comprehensive evaluation on established biomedical benchmarks, we demonstrate that OpenBioNER surpasses state-of-the-art baselines, including specialized 7B NER LLMs and GPT-4o, achieving up to 10% higher F1 scores while using 110M parameters only. Moreover, OpenBioNER outperforms existing small-scale models that match textual spans with entity types rather than descriptions, both in terms of accuracy and computational efficiency.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;a target=&quot;_blank&quot; class=&quot;link&quot; href=&quot;https://aclanthology.org/2025.findings-naacl.47/&quot;&gt;Check out the paper!&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">We are proud to announce that our group will be at NAACL 2025 with 1 long paper in the Findings Track! Catch us in Albuquerque, New Mexico to learn more about open-domain biomedical NER.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://2025.naacl.org/assets/images/venue/balloons.jpg" /><media:content medium="image" url="https://2025.naacl.org/assets/images/venue/balloons.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">AI and Law</title><link href="https://disi-unibo-nlp.github.io/development/2025/01/17/ailaw/" rel="alternate" type="text/html" title="AI and Law" /><published>2025-01-17T00:00:00+00:00</published><updated>2025-01-17T00:00:00+00:00</updated><id>https://disi-unibo-nlp.github.io/development/2025/01/17/ailaw</id><content type="html" xml:base="https://disi-unibo-nlp.github.io/development/2025/01/17/ailaw/">&lt;p&gt;UniboNLP has &lt;b&gt;1 long paper published in Artificial Intelligence and Law 2025.&lt;/b&gt; Read to learn more on &lt;b&gt;knowledge distillation and data generation from LLMs for real-world legal QA&lt;/b&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4&gt;Enhancing Legal Question Answering with Data Generation and Knowledge Distillation from Large Language Models&lt;/h4&gt;
&lt;p style=&quot;color:gray; margin-top:-10px;&quot;&gt;by P. Italiani, G. Moro, and L. Ragazzi&lt;/p&gt;

&lt;p&gt;
Legal question answering (LQA) relies on supervised methods to automatically handle law-related queries. These solutions require a significant amount of carefully annotated data for training, which makes the process very costly. Although large language models (LLMs) show promise in zero-shot QA, their computational demands limit their practical use, making specialized small language models (SLMs) more favorable. Furthermore, the growing interest in synthetic data generation has recently surged, spurred by the impressive generation capabilities of LLMs. This paper presents Ace-Attorney, an LLM distillation approach devised to develop LQA data and supervised models without human annotation. Given a textual prompt, a frozen LLM generates artificial examples that are used as knowledge to train a student SLM with an order of magnitude fewer parameters. Taking into account a realistic retrieval-based scenario to fetch the correct document for answer generation, we propose Selective Generative Paradigm, a novel approach designed to improve retrieval efficacy. Extensive experiments demonstrate the effectiveness and efficiency of distilled models on Syn-LeQA, our human-free synthetic dataset, and a public expert-annotated corpus. Notably, by using only a few dozen training samples, our best SLM achieves LLM-comparable performance with ≈1200% less CO2 emissions.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;a target=&quot;_blank&quot; class=&quot;link&quot; href=&quot;https://link.springer.com/article/10.1007/s10506-025-09463-9&quot;&gt;Check out the paper!&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">UniboNLP has 1 long paper published in Artificial Intelligence and Law 2025. Read to learn more on knowledge distillation and data generation from LLMs for real-world legal QA.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://i0.wp.com/algorithmxlab.com/wp-content/uploads/2019/06/Top-10-Applications-of-Artificial-Intelligence-in-Law.jpg?fit=1900%2C1055&amp;ssl=1" /><media:content medium="image" url="https://i0.wp.com/algorithmxlab.com/wp-content/uploads/2019/06/Top-10-Applications-of-Artificial-Intelligence-in-Law.jpg?fit=1900%2C1055&amp;ssl=1" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">IEEE/ACM TASLP</title><link href="https://disi-unibo-nlp.github.io/development/2025/01/03/taslp/" rel="alternate" type="text/html" title="IEEE/ACM TASLP" /><published>2025-01-03T00:00:00+00:00</published><updated>2025-01-03T00:00:00+00:00</updated><id>https://disi-unibo-nlp.github.io/development/2025/01/03/taslp</id><content type="html" xml:base="https://disi-unibo-nlp.github.io/development/2025/01/03/taslp/">&lt;p&gt;UniboNLP has &lt;b&gt;1 long paper published in IEEE/ACM Transactions on Audio, Speech, and Language Processing 2025.&lt;/b&gt; Read to learn more on &lt;b&gt;multi-document summarization with graphs&lt;/b&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4&gt;Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge&lt;/h4&gt;
&lt;p style=&quot;color:gray; margin-top:-10px;&quot;&gt;by L. Ragazzi, G. Moro, L. Valgimigli, and R. Fiorani&lt;/p&gt;

&lt;p&gt;
Abstractive multi-document summarization aims to generate a comprehensive summary that encapsulates crucial content derived from multiple input documents. Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets. Furthermore, the limited availability of training examples adversely impacts performance, thereby compromising summarization quality in real-world few-shot scenarios. In this paper, we present G-Seek-2, a graph-enhanced approach designed to distill multiple topic-related documents by pinpointing and processing solely the pertinent information. We use a heterogeneous graph to model the input cluster, interconnecting various encoded entities via informative semantic edges. Then, a graph neural network locates the most salient sentences that are provided to a language model to generate the summary. We extensively evaluate G-Seek-2 across seven datasets spanning various domains—including news articles, lawsuits, government reports, and scientific texts—under few-shot settings with a limited training sample size of only 100 examples. The experimental findings demonstrate that our model consistently outperforms advanced summarization baselines, achieving improvements as measured by syntactic and semantic metrics.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;a target=&quot;_blank&quot; class=&quot;link&quot; href=&quot;https://ieeexplore.ieee.org/document/10740791?denied=&quot;&gt;Check out the paper!&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">UniboNLP has 1 long paper published in IEEE/ACM Transactions on Audio, Speech, and Language Processing 2025. Read to learn more on multi-document summarization with graphs.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cdn.prod.website-files.com/656a6f5ca4824808211181c5/6577922ef7937fd8426a089b_An-Introduction-to-Audio-Speech-and-Language-Processing-cropped-1-1.jpeg" /><media:content medium="image" url="https://cdn.prod.website-files.com/656a6f5ca4824808211181c5/6577922ef7937fd8426a089b_An-Introduction-to-Audio-Speech-and-Language-Processing-cropped-1-1.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">EMNLP 2024</title><link href="https://disi-unibo-nlp.github.io/development/2024/10/17/emnlp24/" rel="alternate" type="text/html" title="EMNLP 2024" /><published>2024-10-17T22:25:00+00:00</published><updated>2024-10-17T22:25:00+00:00</updated><id>https://disi-unibo-nlp.github.io/development/2024/10/17/emnlp24</id><content type="html" xml:base="https://disi-unibo-nlp.github.io/development/2024/10/17/emnlp24/">&lt;p&gt;We are glad to share that that our group will be at EMNLP 2024 with &lt;b&gt;1 long paper in the Main Track!&lt;/b&gt; Catch us in &lt;b&gt;Miami, Florida&lt;/b&gt; to learn more about &lt;b&gt;data generation for multimodal computational fact-checking&lt;/b&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4&gt;Unknown Claims: Generation of Fact-Checking Training Examples from Unstructured and Structured Data&lt;/h4&gt;
&lt;p style=&quot;color:gray; margin-top:-10px;&quot;&gt;by JF Bussotti, L. Ragazzi, G. Frisoni, G. Moro, and P. Papotti&lt;/p&gt;
&lt;p&gt;
Computational fact-checking (FC) relies on supervised models to verify claims based on given evidence, requiring a resource-intensive process to annotate large volumes of training data. We introduce Unown, a novel framework that generates training instances for FC systems automatically using both textual and tabular content. Unown selects relevant evidence and generates supporting and refuting claims with advanced negation artifacts. Designed to be flexible, Unown accommodates various strategies for evidence selection and claim generation, offering unparalleled adaptability. We comprehensively evaluate Unown on both text-only and table+text benchmarks, including Feverous, SciFact, and MMFC, a new multi-modal FC dataset. Our results prove that Unown examples are of comparable quality to expert-labeled data, even enabling models to achieve up to 5% higher accuracy. The code, data, and models are available at https://github.com/disi-unibo-nlp/unown.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;a target=&quot;_blank&quot; class=&quot;link&quot; href=&quot;https://aclanthology.org/2024.emnlp-main.675/&quot;&gt;Check out the paper!&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">We are glad to share that that our group will be at EMNLP 2024 with 1 long paper in the Main Track! Catch us in Miami, Florida to learn more about data generation for multimodal computational fact-checking.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://github.com/acl-org/emnlp-2024/blob/main/assets/images/miami/EMNLP_2024_Website_Image.png?raw=true" /><media:content medium="image" url="https://github.com/acl-org/emnlp-2024/blob/main/assets/images/miami/EMNLP_2024_Website_Image.png?raw=true" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">AI and Law</title><link href="https://disi-unibo-nlp.github.io/development/2024/09/29/ailaw/" rel="alternate" type="text/html" title="AI and Law" /><published>2024-09-29T00:00:00+00:00</published><updated>2024-09-29T00:00:00+00:00</updated><id>https://disi-unibo-nlp.github.io/development/2024/09/29/ailaw</id><content type="html" xml:base="https://disi-unibo-nlp.github.io/development/2024/09/29/ailaw/">&lt;p&gt;UniboNLP has &lt;b&gt;2 long papers published in Artificial Intelligence and Law 2024.&lt;/b&gt; Read to learn more on &lt;b&gt;Italian text summarization of constitutional articles&lt;/b&gt; and &lt;b&gt;multilingual case summarization&lt;/b&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4&gt;LAWSUIT: a LArge expert-Written SUmmarization dataset of ITalian constitutional court verdicts&lt;/h4&gt;
&lt;p style=&quot;color:gray; margin-top:-10px;&quot;&gt;by L. Ragazzi, G. Moro, S. Guidi, and G. Frisoni&lt;/p&gt;

&lt;p&gt;
Large-scale public datasets are vital for driving the progress of abstractive summarization, especially in law, where documents have highly specialized jargon. However, the available resources are English-centered, limiting research advancements in other languages. This paper introduces LAWSUIT, a collection of 14K Italian legal verdicts with expert-authored abstractive maxims drawn from the Constitutional Court of the Italian Republic. LAWSUIT presents an arduous task with lengthy source texts and evenly distributed salient content. We offer extensive experiments with sequence-to-sequence and segmentation-based approaches, revealing that the latter achieve better results in full and few-shot settings. We openly release LAWSUIT to foster the development and automation of real-world legal applications.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;a target=&quot;_blank&quot; class=&quot;link&quot; href=&quot;https://link.springer.com/article/10.1007/s10506-024-09414-w&quot;&gt;Check out the paper!&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4&gt;Multi-Language Transfer Learning for Low-Resource Legal Case Summarization&lt;/h4&gt;
&lt;p style=&quot;color:gray; margin-top:-10px;&quot;&gt;by G. Moro, N. Piscaglia, L. Ragazzi, and P. Italiani&lt;/p&gt;

&lt;p&gt;
Analyzing and evaluating legal case reports are labor-intensive tasks for judges and lawyers, who usually base their decisions on report abstracts, legal principles, and commonsense reasoning. Thus, summarizing legal documents is time-consuming and requires excellent human expertise. Moreover, public legal corpora of specific languages are almost unavailable. This paper proposes a transfer learning approach with extractive and abstractive techniques to cope with the lack of labeled legal summarization datasets, namely a low-resource scenario. In particular, we conducted extensive multi- and cross-language experiments. The proposed work outperforms the state-of-the-art results of extractive summarization on the Australian Legal Case Reports dataset and sets a new baseline for abstractive summarization. Finally, syntactic and semantic metrics assessments have been carried out to evaluate the accuracy and the factual consistency of the machine-generated legal summaries.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;a target=&quot;_blank&quot; class=&quot;link&quot; href=&quot;https://link.springer.com/article/10.1007/s10506-023-09373-8&quot;&gt;Check out the paper!&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">UniboNLP has 2 long papers published in Artificial Intelligence and Law 2024. Read to learn more on Italian text summarization of constitutional articles and multilingual case summarization.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://i0.wp.com/algorithmxlab.com/wp-content/uploads/2019/06/Top-10-Applications-of-Artificial-Intelligence-in-Law.jpg?fit=1900%2C1055&amp;ssl=1" /><media:content medium="image" url="https://i0.wp.com/algorithmxlab.com/wp-content/uploads/2019/06/Top-10-Applications-of-Artificial-Intelligence-in-Law.jpg?fit=1900%2C1055&amp;ssl=1" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">ACL 2024</title><link href="https://disi-unibo-nlp.github.io/development/2024/05/17/acl24/" rel="alternate" type="text/html" title="ACL 2024" /><published>2024-05-17T13:55:00+00:00</published><updated>2024-05-17T13:55:00+00:00</updated><id>https://disi-unibo-nlp.github.io/development/2024/05/17/acl24</id><content type="html" xml:base="https://disi-unibo-nlp.github.io/development/2024/05/17/acl24/">&lt;p&gt;We are delighted to announce that we will be at ACL 2024 with &lt;b&gt;2 long papers in the Main and Findings Track!&lt;/b&gt; Catch us in &lt;b&gt;Bangkok, Thailand&lt;/b&gt; to learn more about &lt;b&gt;open-domain QA&lt;/b&gt; and &lt;b&gt;scientific text summarization&lt;/b&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4&gt;To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering&lt;/h4&gt;
&lt;p style=&quot;color:gray; margin-top:-10px;&quot;&gt;by G. Frisoni, A. Cocchieri, A. Presepi, G. Moro, and Z. Meng&lt;/p&gt;

&lt;p&gt;
Medical open-domain question answering demands substantial access to specialized knowledge. Recent efforts have sought to decouple knowledge from model parameters, counteracting architectural scaling and allowing for training on common low-resource hardware. The retrieve-then-read paradigm has become ubiquitous, with model predictions grounded on relevant knowledge pieces from external repositories such as PubMed, textbooks, and UMLS. An alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting. As a result, “to generate or to retrieve” is the modern equivalent of Hamlet’s dilemma. This paper presents MedGENIE, the first generate-then-read framework for multiple-choice question answering in medicine. We conduct extensive experiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical perspective by assuming a maximum of 24GB VRAM. MedGENIE sets a new state-of-the-art in the open-book setting of each testbed, allowing a small-scale reader to outcompete zero-shot closed-book 175B baselines while using up to 706x fewer parameters. Our findings reveal that generated passages are more effective than retrieved ones in attaining higher accuracy.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;a target=&quot;_blank&quot; class=&quot;link&quot; href=&quot;https://aclanthology.org/2024.acl-long.533/&quot;&gt;Check out the paper!&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4&gt;What Are You Token About? Differentiable Perturbed Top-k Token Selection for Scientific Document Summarization&lt;/h4&gt;
&lt;p style=&quot;color:gray; margin-top:-10px;&quot;&gt;by L. Ragazzi, P. Italiani, G. Moro, and M. Panni&lt;/p&gt;

&lt;p&gt;
Scientific document summarization aims to condense complex and long articles in both technical and plain-language terms to facilitate the accessibility and dissemination of scientific findings. Existing datasets suffer from a deficiency in source heterogeneity, as their data predominantly stem from a single common resource, hindering effective model training and generalizability. First, we introduce SciLay, a novel dataset that includes documents from multiple natural science journals with expert-authored technical and lay summaries. Second, we propose PrunePert, a new transformer-based model that incorporates a differentiable perturbed top-k encoder layer to prune irrelevant tokens in end-to-end learning. Experimental results show that our model achieves a nearly 2x speed-up compared to a state-of-the-art linear transformer, remaining comparable in effectiveness. Additional examinations underscore the importance of employing a training dataset that includes different sources to enhance the generalizability of the models. Code is available at https://github.com/disi-unibo-nlp/sci-lay.
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;a target=&quot;_blank&quot; class=&quot;link&quot; href=&quot;https://aclanthology.org/2024.findings-acl.561/&quot;&gt;Check out the paper!&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">We are delighted to announce that we will be at ACL 2024 with 2 long papers in the Main and Findings Track! Catch us in Bangkok, Thailand to learn more about open-domain QA and scientific text summarization.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://github.com/acl-org/acl-2024/blob/main/assets/images/bangkok/bangkok-banner.jpeg?raw=true" /><media:content medium="image" url="https://github.com/acl-org/acl-2024/blob/main/assets/images/bangkok/bangkok-banner.jpeg?raw=true" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">LLM Slide Deck</title><link href="https://disi-unibo-nlp.github.io/development/2024/04/17/llm-slides/" rel="alternate" type="text/html" title="LLM Slide Deck" /><published>2024-04-17T00:00:00+00:00</published><updated>2024-04-17T00:00:00+00:00</updated><id>https://disi-unibo-nlp.github.io/development/2024/04/17/llm-slides</id><content type="html" xml:base="https://disi-unibo-nlp.github.io/development/2024/04/17/llm-slides/">&lt;p&gt;🤖 We are proud to share that our group has curated a &lt;b&gt;short review of the state-of-the-art within the Large Language Model (LLM) literature&lt;/b&gt;. Dive into the cutting-edge world of LLMs with our latest slide deck!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Developed in Text Mining and NLP, LLMs are driving a breakthrough in AI and human-machine interaction. They impact a multitude of tasks and domains and are poised to revolutionize society as a whole. In a future where tokens function as currency, mastering and implementing these subjects is an imperative skill in academic and industrial contexts.&lt;/p&gt;

&lt;p&gt;🔎 In our review, explore topics such as:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;LLM applications&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Preliminaries&lt;/strong&gt; (e.g., transformers, scaling laws, emerging abilities)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Open issues&lt;/strong&gt; (e.g., black-box behavior, training costs, hallucinations, knowledge recency and editing, lexical superficiality, human reporting bias, reasoning inconsistency)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Developer perspective&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Models&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Datasets&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Prompting&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Training techniques&lt;/em&gt; (e.g., instruction fine-tuning, adapter fine-tuning, mixture-of-experts)&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Optimization&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Retrieval-augmented generation&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, we briefly introduce the contributions of our group in the last three years.&lt;/p&gt;

&lt;p&gt;Ready to level up your understanding of LLMs?&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;a class=&quot;link&quot; href=&quot;https://www.dropbox.com/scl/fi/hdn98k7paq0yca91zovow/LLM_NLP_Group_DISI_UniBO_Cesena_Campus_v2.pdf?rlkey=d2yw9x8l25glo3317i7cznov4&amp;amp;dl=0&quot;&gt;Link to the slides&lt;/a&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;a class=&quot;link&quot; href=&quot;https://www.dropbox.com/scl/fi/rtry6md8xzasr9lsyxv88/LLM_NLP_Group_DISI_UniBO_Cesena_Campus_2x.pdf?rlkey=5xb27etx4o4xfle94ubn0cu2p&amp;amp;dl=0&quot;&gt;Compact printable version&lt;/a&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;#NLP #AI #GenerativeAI #LLMs #Research #Innovation&lt;/p&gt;</content><author><name></name></author><summary type="html">🤖 We are proud to share that our group has curated a short review of the state-of-the-art within the Large Language Model (LLM) literature. Dive into the cutting-edge world of LLMs with our latest slide deck!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://drive.google.com/thumbnail?sz=w1920&amp;id=1eVN7Cw3mBZfrQXwYGT0jNjoAMMfPsnt5" /><media:content medium="image" url="https://drive.google.com/thumbnail?sz=w1920&amp;id=1eVN7Cw3mBZfrQXwYGT0jNjoAMMfPsnt5" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>